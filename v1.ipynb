{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121 running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"{torch.__version__} running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Run for Image Classification\n",
    "Your problem needs to fulfill the following criteria.\n",
    "1. It is an image classification problem.\n",
    "2. You supply marked training images and marked validation images.\n",
    "\n",
    "Within those, the run is flexible and adapts itself to your problem.\n",
    "Now, please describe your images and problem by setting those global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28 # <-- number of width-pixels\n",
    "IMAGE_HEIGHT = 28 # <-- number of height-pixels\n",
    "COLOUR_CHANNEL_COUNT = 1 # <-- RGB images would have 3\n",
    "CLASSIFICATION_CATEGORIES_COUNT = 10 # <-- the amount of possible categories of which each image shall be marked with one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Example Data\n",
    "To check the code, we prepare example data: Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data (and not testing data)\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
    "print(f\"train_data.classes = {train_data.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dl_f_mnist.batch_size = 32\n",
      "len(next(iter(train_dl_f_mnist))) = 2\n",
      "next(iter(train_dl_f_mnist))[0].shape = torch.Size([32, 1, 28, 28])\n",
      "[32, 1, 28, 28] = [MINI_BATCH_SIZE, COLOUR_CHANNEL_COUNT, IMAGE_WIDTH, IMAGE_HEIGHT]\n",
      "len(train_dl_f_mnist) = 1875, len(test_dl_f_mnist) = 313\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "MINI_BATCH_SIZE = 32 # constant for now\n",
    "\n",
    "# Turn datasets into iterables (batches), shuffeling train data every epoch (test data not)\n",
    "train_dl_f_mnist = DataLoader(train_data, batch_size=MINI_BATCH_SIZE, shuffle=True)\n",
    "test_dl_f_mnist = DataLoader(test_data, batch_size=MINI_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"train_dl_f_mnist.batch_size = {train_dl_f_mnist.batch_size}\") \n",
    "print(f\"len(next(iter(train_dl_f_mnist))) = {len(next(iter(train_dl_f_mnist)))}\") \n",
    "print(f\"next(iter(train_dl_f_mnist))[0].shape = {next(iter(train_dl_f_mnist))[0].shape}\") \n",
    "print(f\"[{MINI_BATCH_SIZE}, {COLOUR_CHANNEL_COUNT}, {IMAGE_WIDTH}, {IMAGE_HEIGHT}] = [MINI_BATCH_SIZE, COLOUR_CHANNEL_COUNT, IMAGE_WIDTH, IMAGE_HEIGHT]\")\n",
    "print(f\"len(train_dl_f_mnist) = {len(train_dl_f_mnist)}, len(test_dl_f_mnist) = {len(test_dl_f_mnist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 2 # index in the batch, 0 .. 31\n",
    "train_features_batch, train_labels_batch = next(iter(train_dl_f_mnist))\n",
    "print(f\"Image shape: {train_features_batch[image_index].shape}\")\n",
    "plt.imshow(train_features_batch[image_index].squeeze(), cmap=\"gray\") # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(str(train_labels_batch[image_index].item())+\" i.e. \"+train_data.classes[train_labels_batch[image_index].item()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Individuals' Class\n",
    "- The Hyperparamters should be passed to the constructor in a way that is both convenient for GP and for PyTorch.\n",
    "- I think I want to define a class for one `nn.Sequential` 2d-block\n",
    "    - All possible instances should be concatenable with all possible instances\n",
    "- Then, an individual is built from the concatenation of many such blocks, plus data preparation and final f.c. 2d_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\\nprint(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded later in the notebook...\n",
    "class Sequential_block_2d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_channels: int, # the number of output neurons after the full block\n",
    "                 in_channels: int = 1, # should not be set here, but is set in Individual's __init__()\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=conv_kernel_size,\n",
    "                      stride=conv_stride,\n",
    "                      padding=conv_padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size,\n",
    "                         stride=pool_stride,\n",
    "                         padding=pool_padding)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# generate some testing blocks (TODO: write actual unit tests)\n",
    "testBlock1 = Sequential_block_2d(in_channels=1,out_channels=5)\n",
    "testBlock2 = Sequential_block_2d(in_channels=5,out_channels=3)\n",
    "torch.manual_seed(42)\n",
    "'''\n",
    "testX = torch.randn(COLOUR_CHANNEL_COUNT,IMAGE_WIDTH,IMAGE_HEIGHT).to(device)\n",
    "'''\n",
    "print(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\n",
    "print(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' superseded later in the notebook...\\n# class for image classification individuals\\nclass NN_individual(nn.Module):\\n    def __init__(self, blocks_2d: list[Sequential_block_2d],\\n                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\\n                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\\n                 fin_res: int = 5): # output dimension of the final max pooling \\n                                    # producing size (fin_res * fin_res)\\n        super().__init__()\\n        self.blocks_2d = blocks_2d\\n        # add a final max pool\\n        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\\n        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\\n        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\\n        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\\n                      out_features = out_dimensions)\\n    def forward(self, x):\\n        for i in range(len(self.blocks_2d)):\\n            x = self.blocks_2d[i](x)\\n        x = self.last_max_pool_adaptive(x)\\n        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\\n        x = self.flatten(x)\\n        print(f\"flatten output shape is {x.shape}\")\\n        x = self.lin(x)\\n        print(f\"lin output shape is {x.shape}\")\\n        return x\\n    \\ntestIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\\ntestIndividual(testX)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded later in the notebook...\n",
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, blocks_2d: list[Sequential_block_2d],\n",
    "                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\n",
    "                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\n",
    "                 fin_res: int = 5): # output dimension of the final max pooling \n",
    "                                    # producing size (fin_res * fin_res)\n",
    "        super().__init__()\n",
    "        self.blocks_2d = blocks_2d\n",
    "        # add a final max pool\n",
    "        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\n",
    "        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\n",
    "                      out_features = out_dimensions)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.last_max_pool_adaptive(x)\n",
    "        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lin(x)\n",
    "        print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\n",
    "testIndividual(testX)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' superseded\\n# this way, adjacent genes (= 2d_blocks) need to have the correct dimensions\\n# e.g. the following will error:\\nbadIndividual = NN_individual([Sequential_block_2d(in_channels=1,out_channels=2), Sequential_block_2d(in_channels=3, out_channels=5)])\\ntry:\\n    print(badIndividual(testX))\\nexcept:\\n    print(\"out_channels of the first needs to match in_channels of the second!\")\\n\\n# but there\\'s more redundancy:\\the first gene needs to have the same number of in_channels as there are colour channels\\n# e.g. the following will error:\\nbadIndividual = NN_individual([Sequential_block_2d(in_channels=COLOUR_CHANNEL_COUNT + 1,out_channels=5)])\\ntry:\\n    print(badIndividual(testX))\\nexcept:\\n    print(\"in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\")\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded\n",
    "# this way, adjacent genes (= 2d_blocks) need to have the correct dimensions\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=1,out_channels=2), Sequential_block_2d(in_channels=3, out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"out_channels of the first needs to match in_channels of the second!\")\n",
    "\n",
    "# but there's more redundancy:\\the first gene needs to have the same number of in_channels as there are colour channels\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=COLOUR_CHANNEL_COUNT + 1,out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brainstorm on How to Encode Individuals\n",
    "We need to talk about this right now because we want to adapt our `NN_individual.blocks_2d` definition according to it.\n",
    "The options are:\n",
    "1. We specify `Sequential_block_2d.in_channels` and `~.out_channels` separately for each individual and only allow concatenation if the criteria are met. This is probably not super clever...\n",
    "2. Genotype-closure: The parameters that are adapted through GP will never leave the space of syntacticly correct indivuals\n",
    "    - The first gene is not allowed to choose `~.in_channels`, it must match `COLOUR_CHANNEL_COUNT`\n",
    "    - Every gene but the first is not allowed to choose `~.in_channels`, it must match `~.out_channels` of the prior gene\n",
    "    - How will this change the gene class `Sequential_block_2d`?\n",
    "        - Set `Sequential_block_2d.in_channels` only programatically, in `NN_individual.__init__()`\n",
    "        - Don't even let the genes inherit from `nn.Module`, only the individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_channels = 3 <-- (28 x 28)\n",
      "conv_2d (kernel, stride, padding) =\t(3, 1, 1) --> (28 x 28)\n",
      "max_pool_2d (kernel, stride, padding) =\t(2, 2, 0) --> (14 x 14)\n"
     ]
    }
   ],
   "source": [
    "''' Calculate the image size after a layer has been applied\n",
    "    assume all operations to be x/y symmetric '''\n",
    "def output_size(h_in, kernel_size, stride, padding):\n",
    "    h_out = (h_in + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    #w_out = (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    return h_out # = w_out\n",
    "\n",
    "# class for the genetic information of one 2d block\n",
    "class Gene_2d_block:\n",
    "    def __init__(self,\n",
    "                 input_image_size: int,\n",
    "                 out_channels: int,\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        self.input_image_size = input_image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = None\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_stride = conv_stride\n",
    "        self.conv_padding = conv_padding\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.pool_padding = pool_padding\n",
    "\n",
    "        self.after_conv_image_size = output_size(input_image_size, conv_kernel_size, conv_stride, conv_padding)\n",
    "        self.output_image_size = output_size(self.after_conv_image_size, pool_kernel_size, pool_stride, pool_padding)\n",
    "\n",
    "    def toString(self, tab_count: int = 0):\n",
    "        indentation = \"\"\n",
    "        for tab in range(tab_count): indentation += f\"\\t\"\n",
    "        return f\"{indentation}out_channels = {self.out_channels} <-- ({self.input_image_size} x {self.input_image_size})\\n\"+\\\n",
    "        f\"{indentation}conv_2d (kernel, stride, padding) =\\t({self.conv_kernel_size}, {self.conv_stride}, {self.conv_padding}) --> ({self.after_conv_image_size} x {self.after_conv_image_size})\\n\"+\\\n",
    "        f\"{indentation}max_pool_2d (kernel, stride, padding) =\\t({self.pool_kernel_size}, {self.pool_stride}, {self.pool_padding}) --> ({self.output_image_size} x {self.output_image_size})\"\n",
    "\n",
    "print(Gene_2d_block(IMAGE_WIDTH, 3).toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this is supserseded below...\\n# class for image classification individuals\\nclass NN_individual(nn.Module):\\n    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \\n        super().__init__()\\n        self.name=name # a name for easier tracking inside a GP run\\n        # build the full sequential from the gene information (genes_2d_block)\\n        self.blocks_2d = nn.Sequential()\\n        # the first 2d_block needs to have as many in_channels as there are colour channels\\n        # the others need to have as in_channels the number of out_channels from the previous block\\n        for i in range(len(genes_2d_block)):\\n            if i == 0:\\n                in_channels = COLOUR_CHANNEL_COUNT\\n            else:\\n                in_channels = genes_2d_block[i-1].out_channels\\n            self.blocks_2d.append(nn.Sequential(\\n                nn.Conv2d(in_channels=in_channels,\\n                    out_channels=genes_2d_block[i].out_channels,\\n                    kernel_size=genes_2d_block[i].conv_kernel_size,\\n                    stride=genes_2d_block[i].conv_stride,\\n                    padding=genes_2d_block[i].conv_padding),\\n                nn.ReLU(),\\n                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\\n                    stride=genes_2d_block[i].pool_stride,\\n                    padding=genes_2d_block[i].pool_padding)))\\n        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\\n        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\\n    def forward(self, x):\\n        for i in range(len(self.blocks_2d)):\\n            x = self.blocks_2d[i](x)\\n        x = self.flatten(x)\\n        #print(f\"flatten output shape is {x.shape}\")\\n        x = self.lazyLin(x)\\n        #print(f\"lin output shape is {x.shape}\")\\n        return x\\n    \\ntestIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\\ntestIndividual, testIndividual(testX)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' this is supserseded below...\n",
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \n",
    "        super().__init__()\n",
    "        self.name=name # a name for easier tracking inside a GP run\n",
    "        # build the full sequential from the gene information (genes_2d_block)\n",
    "        self.blocks_2d = nn.Sequential()\n",
    "        # the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "        # the others need to have as in_channels the number of out_channels from the previous block\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            self.blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lazyLin(x)\n",
    "        #print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Code\n",
    "Now we need to decide the genetic stuff.\n",
    "1. Initial Population\n",
    "2. Fitness Measure\n",
    "3. Selection\n",
    "4. Genetic Operators\n",
    "    - Cloning or Crossover\n",
    "    - Mutation\n",
    "    \n",
    "### Hyperparameter-landscape is vast. Here's a list:\n",
    "- Net architecture\n",
    "    - kind of layers, number of layers\n",
    "        - for convolution/pooling: kernel size, stride, padding, (dilation) **[implemented]**\n",
    "    - number of neurons per layer\n",
    "    - activation function for each layer\n",
    "- cost function\n",
    "    - base term (e.g. square cost, log-likelihood, cross-entropy, ect.)\n",
    "    - toppings \n",
    "        - regularization of weights (L2, L1, dropout, etc.)\n",
    "- weights and biases optimization technique (= optimizer)\n",
    "    - SGD (= stochastic gradient descent)\n",
    "    - Hessian technique, i.e. momentum-based descent\n",
    "    - PyTorch's various other (e.g. *Adam* optimizer)\n",
    "- learning parameters\n",
    "    - η ... learning rate\n",
    "        - constant, or epoch-dependent, or accuracy-dependent, or a mix\n",
    "    - \\# of epochs\n",
    "        - constant, or early stopping\n",
    "    - (`mini_batch_size` - this one might be canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Within-One-Gen-Constant Hyperparameters in `NN_individual`\n",
    "We now bake:\n",
    "- the individual-specific, hyperparameters (that don't change within one gen)\n",
    "- the fitness dictionaries\n",
    "\n",
    "into parameters of the `NN_individual` class.\n",
    "\n",
    "We construct this class from a `NN_dna` class that contains all the genetic information necessary to build the `NN_individual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A helper that gets an otpimizer_string where the given chars need to appear and be followed by a signed integer.\n",
    "    E.g. optimizer_string=\"SGD@λ3μ-2χ0\" and chars=['λ','χ','μ'] will produce the list [3.0, 0.0, -2.0]'''\n",
    "def parse_optimizer_string(optimizer_string, chars = ['λ','μ']) -> list[float]:\n",
    "    exponents = []\n",
    "    for char in chars:\n",
    "        match = re.search(fr'{char}([-+]?\\d+)', optimizer_string) # Use regex to find the values following the char\n",
    "        if match: exponents.append(float(match.group(1))) # Extract and convert these values to floats\n",
    "        else: raise ValueError(f\"Could not find '{char}' in the string.\")\n",
    "    return exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class containing various genes (with their vanilla values) and the G2P mappings\n",
    "class NN_dna():\n",
    "    def __init__(self,\n",
    "                 blocks_2d: list[Gene_2d_block] = [],\n",
    "                 optimizer: int = 0,\n",
    "                 lr: float = .1,\n",
    "                 loss_fn: int = 0,\n",
    "                 ) -> None:\n",
    "        self.blocks_2d_gene = blocks_2d\n",
    "        self.optimizer_gene = optimizer\n",
    "        self.lr = lr\n",
    "        self.loss_fn_gene = loss_fn\n",
    "    \n",
    "    #### Genotype To Phenotype Mappings (G2P) ####\n",
    "    def blocks_2d_G2P(self):\n",
    "        ''' build 'n return the full sequential from the gene information (genes_2d_block) \n",
    "            the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "            the others need to have as in_channels the number of out_channels from the previous block\n",
    "            there's a nn.Module (Lazy*) that automatically infers the number of in_channels - not used here '''\n",
    "        blocks_2d = nn.Sequential()\n",
    "        for i in range(len(self.blocks_2d_gene)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = self.blocks_2d_gene[i-1].out_channels\n",
    "            blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=self.blocks_2d_gene[i].out_channels,\n",
    "                    kernel_size=self.blocks_2d_gene[i].conv_kernel_size,\n",
    "                    stride=self.blocks_2d_gene[i].conv_stride,\n",
    "                    padding=self.blocks_2d_gene[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.blocks_2d_gene[i].pool_kernel_size,\n",
    "                    stride=self.blocks_2d_gene[i].pool_stride,\n",
    "                    padding=self.blocks_2d_gene[i].pool_padding)))\n",
    "        return blocks_2d\n",
    "        \n",
    "    optimizer_dict = {0: \"SGD@λ0μ0\", 1: \"SGD@λ0μ1\", 2: \"SGD@λ0μ-1\", 3: \"SGD@λ1μ0\", 4: \"SGD@λ1μ1\", 5: \"SGD@λ1μ-1\", 6: \"SGD@λ-1μ0\", 7: \"SGD@λ-1μ1\", 8: \"SGD@λ-1μ-1\", 9: \"adam\"}\n",
    "    def optimizer_G2P(self, model_parameters):\n",
    "        if self.optimizer_dict[self.optimizer_gene].lower().startswith(\"sgd\"): \n",
    "            exps = parse_optimizer_string(self.optimizer_dict[self.optimizer_gene],chars=['λ','μ']) # find the exponents to use from the name I chose in optimizer_dict\n",
    "            return torch.optim.SGD(model_parameters, lr=self.lr, weight_decay=math.exp(exps[0]), momentum=math.exp(exps[1])) # use Euler's e as base for the exponents\n",
    "        if self.optimizer_gene == 9: return torch.optim.Adam(model_parameters, lr=self.lr)\n",
    "        raise ValueError(f\"'{self.optimizer_gene}' is not a gene for which we have an optimizer encoded.\")\n",
    "    \n",
    "    # a loss function needs to come with information of how targets need to be encoded, either y = 2 (1hot=False) or y = [0,0,1,0,...] (1hot=True)\n",
    "    loss_fn_dict = {0: {'name': \"CE\", '1hot': False}, 1: {'name': \"L1\", '1hot': True}, 2: {'name': \"Hub@δ=.1\", '1hot': True}, 3: {'name': \"Hub@δ=1\", '1hot': True}, 4: {'name': \"Hub@δ=10\", '1hot': True}}\n",
    "    def loss_fn_G2P(self):\n",
    "        if self.loss_fn_gene == 0: return nn.CrossEntropyLoss()\n",
    "        ''' The following loss functions should only be used with one-hot encoded targets.\n",
    "            This means that instead of e.g. y = 3, we need y = [0,0,0,1,0,0,0,0,0,0] if CATEGORIES_COUNT = 10.\n",
    "            So all the (wrong) predictions - i.e. x_0,x_1,x_2,x_4,x_5,... - are ignored in the loss calculation.\n",
    "            This is why usually CrossEn is used, according to ChatGPT'''\n",
    "        if self.loss_fn_gene == 1: return nn.L1Loss()\n",
    "        if self.loss_fn_gene == 2: return nn.HuberLoss(delta=.1)\n",
    "        if self.loss_fn_gene == 3: return nn.HuberLoss(delta=1) # same as SmoothL1Loss\n",
    "        if self.loss_fn_gene == 4: return nn.HuberLoss(delta=10)\n",
    "        raise ValueError(f\"'{self.loss_fn_gene}' is not a gene for which we have a loss function encoded.\")\n",
    "\n",
    "    def toString(self):\n",
    "        s = \"(\"\n",
    "        for i, block in enumerate(self.blocks_2d_gene):\n",
    "            s += f\"{block.out_channels},\"                           # e.g. (1,7,\n",
    "        s = s[:-1]                                                  # e.g. (1,7\n",
    "        s += f\") {self.optimizer_dict[self.optimizer_gene]}\"        # e.g. (1,7) SGD\n",
    "        s += f\" & {self.loss_fn_dict[self.loss_fn_gene]['name']}\"   # e.g. (1,7) SGD & CrossEn\n",
    "        s += f\" @ {self.lr:.2g}\"                                    # e.g. (1,7) SGD & CrossEn @ 1.1\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NN_individual(\n",
       "   (blocks_2d): Sequential()\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (lazyLin): Linear(in_features=784, out_features=10, bias=True)\n",
       "   (loss_fn): CrossEntropyLoss()\n",
       " ),\n",
       " tensor([[ 0.3682,  0.7262, -0.1179,  0.1921,  0.9131,  0.4694, -0.1444, -0.3196,\n",
       "           0.5976,  0.4390]], device='cuda:0', grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' class for image classification individuals\n",
    "    Essentially, it converts NN_dna (provided to __init__)\n",
    "    into a working NN with a forward method\n",
    "'''\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dna: NN_dna = NN_dna(),    # <- contains all genes\n",
    "                 name: str = \"nn0\",         # <- name unique within a population/generation\n",
    "                 device = device):\n",
    "        super().__init__()\n",
    "        self.dna = dna\n",
    "        self.blocks_2d = dna.blocks_2d_G2P()\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1) # default is: start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "        self.name = name\n",
    "        self.lr = dna.lr\n",
    "        self.optimizer = dna.optimizer_G2P(self.parameters())\n",
    "        self.loss_fn = dna.loss_fn_G2P()\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.acc = 0\n",
    "        self.running_acc = 0\n",
    "        self.train_losses = {}\n",
    "        self.test_losses = {}\n",
    "        self.accs = {}\n",
    "        self.elapsed_training_time = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lazyLin(x)\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual()#genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize, Visualize, Visualize\n",
    "Create a class called population of which an instance acts as an array of `NN_individual`s with extra functionality that regards the whole population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created by Chat\n",
    "class NN_population:\n",
    "    ### Magic Methods ###\n",
    "    def __init__(self, individuals: list[NN_individual]): self.individuals = individuals\n",
    "    def __getitem__(self, index): return self.individuals[index]  # magic pop[i] access\n",
    "    def __len__(self): return len(self.individuals)  # magic len(pop)\n",
    "    def __setitem__(self, index, value): self.individuals[index] = value  # magic pop[i] = value\n",
    "    def __iter__(self): return iter(self.individuals)  # magic for-iterations\n",
    "    \n",
    "    def plot_accs(self, elapsed_time = 0):\n",
    "        plt.figure(figsize=(15, 6))  # Set the figure size\n",
    "        for ind in sorted(self.individuals, key=lambda ind: ind.running_acc, reverse=True):\n",
    "            x = list(ind.accs.keys())  # Extract the epoch/batch labels (x-axis)\n",
    "            y = [float(val.cpu().item()*100) for val in ind.accs.values()]  # Convert tensors to floats\n",
    "            # Plot each individual's accuracies\n",
    "            plt.plot(x, y, marker='o', linestyle='-', label=f\"{ind.name} [{ind.dna.toString()}] ({ind.running_acc:.2f} within {ind.elapsed_training_time:.1f}s)\")\n",
    "        plt.xlabel('Epoch@Batch')  # Label for the x-axis\n",
    "        plt.ylabel('Accuracy [%]')     # Label for the y-axis\n",
    "        extra_title = \"\" if elapsed_time == 0 else f\" (took {elapsed_time:.2f}s)\"\n",
    "        plt.title('Accuracy per Epoch and Batch' + extra_title)  # Title of the plot\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "        plt.grid(True)  # Show grid\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1)) # legend on the right\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust plot area size to leave space for the legend\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_random_2d_block(input_image_size, max_kernel_size: int) -> Gene_2d_block:\n",
    "    conv_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size))) # kernel must be smaller than image size!\n",
    "    conv_stride=random.randint(1,conv_kernel_size)\n",
    "    conv_padding=random.randint(0,conv_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "    after_conv_i_s = output_size(input_image_size, conv_kernel_size, conv_stride, conv_padding)\n",
    "    pool_kernel_size=min(random.randint(1,min(after_conv_i_s, max_kernel_size)), random.randint(1,min(after_conv_i_s, max_kernel_size)), random.randint(1,min(after_conv_i_s, max_kernel_size)))\n",
    "    if conv_kernel_size == 0 or pool_kernel_size == 0:\n",
    "        print(\"Exception! A kernel size is 0, which is not allowed.\")\n",
    "        print(f\"input_image_size {input_image_size}, max_kernel_size {max_kernel_size}, conv_kernel_size {conv_kernel_size}, conv_stride {conv_stride}, conv_padding {conv_padding}, after_conv_image_size {after_conv_i_s}, pool_kernel_size {pool_kernel_size}\")\n",
    "    pool_stride=max(random.randint(1,pool_kernel_size), random.randint(1,pool_kernel_size))\n",
    "    pool_padding=random.randint(0,pool_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "    return Gene_2d_block(\n",
    "        input_image_size=input_image_size,\n",
    "        out_channels=random.randint(3,15), # not fine-tuned\n",
    "        conv_kernel_size=conv_kernel_size,\n",
    "        conv_padding=conv_padding,\n",
    "        conv_stride=conv_stride,\n",
    "        pool_kernel_size=pool_kernel_size,\n",
    "        pool_padding=pool_padding,\n",
    "        pool_stride=pool_stride\n",
    "    )\n",
    "\n",
    "def update_and_check_2d_block_stack(gene_2d_blocks: list[Gene_2d_block]):\n",
    "    protocol = \"\"\n",
    "    if len(gene_2d_blocks) == 0: return \"no block in the stack\"\n",
    "    if gene_2d_blocks[0].input_image_size != IMAGE_HEIGHT:\n",
    "        gene_2d_blocks[0].input_image_size != IMAGE_HEIGHT\n",
    "        protocol += f\"block[0]'s input_image_size was set to IMAGE_HEIGHT ({IMAGE_HEIGHT}), \" \n",
    "    for i, block in enumerate(gene_2d_blocks):\n",
    "        if i > 0 and block.input_image_size != gene_2d_blocks[i-1].output_image_size:\n",
    "            block.input_image_size = gene_2d_blocks[i-1].output_image_size\n",
    "            protocol += f\"block[{i}]'s input_image_size was set to block[{i-1}]'s output_image_size ({block.input_image_size}), \"\n",
    "            block.after_conv_image_size = output_size(block.input_image_size, block.conv_kernel_size, block.conv_stride, block.conv_padding) # update the effective image size after convolution\n",
    "        if block.input_image_size < block.conv_kernel_size:\n",
    "            block.conv_kernel_size = block.input_image_size\n",
    "            protocol += f\"block[{i}]'s conv_kernel_size was decreased to input_image_size ({block.conv_kernel_size}), \"\n",
    "        if block.conv_padding > block.conv_kernel_size // 2:\n",
    "            block.conv_padding = block.conv_kernel_size // 2\n",
    "            protocol += f\"block[{i}]'s conv_padding was decreased to conv_kernel_size//2 ({block.conv_kernel_size // 2}), \"\n",
    "        if block.after_conv_image_size < block.pool_kernel_size:\n",
    "            block.pool_kernel_size = block.after_conv_image_size\n",
    "            protocol += f\"block[{i}]'s pool_kernel_size was decreased to after_conv_size ({block.pool_kernel_size}), \"\n",
    "        if block.pool_padding > block.pool_kernel_size // 2:\n",
    "            block.pool_padding = block.pool_kernel_size // 2\n",
    "            protocol += f\"block[{i}]'s pool_padding was decreased to pool_kernel_size//2 ({block.pool_kernel_size // 2}), \"\n",
    "    return protocol\n",
    "    \n",
    "def create_random_population(pop_size: int, \n",
    "                             max_2d_block_count: int = 3, \n",
    "                             max_kernel_size: int = 11,\n",
    "                             name_prefix=\"nn\",\n",
    "                             device=device,\n",
    "                             print_summary: bool = True) -> NN_population:\n",
    "    population = []\n",
    "    for i in range(pop_size):\n",
    "        genes_2d_block = []\n",
    "        input_image_size = IMAGE_HEIGHT # = IMAGE_WIDTH (assumed)\n",
    "        name=name_prefix+str(i)\n",
    "        if print_summary: print(f\"Individual '{name}' <-- ({input_image_size} x {input_image_size})\")\n",
    "        for j in range(random.randint(1, max_2d_block_count)):\n",
    "            if print_summary: print(f\"\\tBlock {j}\")\n",
    "            # create a random conv-pool block and store the corresponding new input_image_size for the block thereafter\n",
    "            block = create_random_2d_block(input_image_size, max_kernel_size)\n",
    "            input_image_size = block.output_image_size\n",
    "            genes_2d_block.append(block)\n",
    "            if print_summary: print(f\"{block.toString(tab_count=2)} --> ({input_image_size} x {input_image_size})\")\n",
    "        dna = NN_dna(blocks_2d=genes_2d_block,\n",
    "                     loss_fn=random.randrange(len(NN_dna.loss_fn_dict)),\n",
    "                     optimizer=random.randrange(len(NN_dna.optimizer_dict)),\n",
    "                    # here you can change the remaining hyperparameters\n",
    "                    )\n",
    "        population.append(NN_individual(dna=dna, name=name, device=device))\n",
    "    return NN_population(population)\n",
    "testPop = create_random_population(pop_size=7, max_2d_block_count=3, print_summary=False)\n",
    "try:\n",
    "    for ind in testPop:\n",
    "        ind.eval()\n",
    "        with torch.inference_mode():\n",
    "            ind(testX)\n",
    "except:\n",
    "    print(\"oh, oh! exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Evaluation\n",
    "We want a population that:\n",
    "- achieves high (validation/test data) accuracy after training\n",
    "    - the final accuracy `acc(NN1(t_final))` of an individual `NN1` is used\n",
    "- trains fast, i.e. takes little CPU time to achieve high accuracy called **Running Accuracy**\n",
    "    - the individual's accuracy `acc(NN1(t))` is summed over given timestamps `t`, like `Σ_t{acc(NN1(t))}`\n",
    "    - possibly we want to value early accuracy more, summing `Σ_t{acc(NN1(t))/t}` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import functional\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_one_batch(ind: NN_individual, # <- model to be trained in-place\n",
    "                          X, y,               # <- train batch, e.g. X.shape = [32, 1, 28, 28]\n",
    "                          ) -> tuple[float, float]:\n",
    "  train_loss = 0\n",
    "  start_time = time.perf_counter() # Start timing\n",
    "  ind.train()\n",
    "  X, y = X.to(ind.device), y.to(ind.device)\n",
    "  y_pred = ind(X)\n",
    "  if NN_dna.loss_fn_dict[ind.dna.loss_fn_gene]['1hot']: # <- does the loss_fn need one-hot encoding?\n",
    "    loss = ind.loss_fn(y_pred, F.one_hot(y, num_classes=CLASSIFICATION_CATEGORIES_COUNT).float())\n",
    "  else:\n",
    "    loss = ind.loss_fn(y_pred, y)\n",
    "  train_loss += loss\n",
    "  ind.optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  ind.optimizer.step()\n",
    "  end_time = time.perf_counter() # Stop timing\n",
    "  return (train_loss, end_time-start_time)\n",
    "\n",
    "def test_model(ind: NN_individual,            # <- model to be tested\n",
    "               test_dl,                       # <- test dataloader (= multiple batches)\n",
    "               ) -> tuple[float, float]:      # -> return (loss_total, acc_total)\n",
    "  loss_total, acc_total = 0, 0\n",
    "  ind.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X, y) in enumerate(test_dl):\n",
    "      X, y = X.to(ind.device), y.to(ind.device)\n",
    "      preds = ind(X)\n",
    "      if NN_dna.loss_fn_dict[ind.dna.loss_fn_gene]['1hot']: # <- does the loss_fn need one-hot encoding?\n",
    "        loss_batch = ind.loss_fn(preds, F.one_hot(y, num_classes=CLASSIFICATION_CATEGORIES_COUNT).float())\n",
    "      else:\n",
    "        loss_batch = ind.loss_fn(preds, y)\n",
    "      loss_total += loss_batch\n",
    "      acc_batch = functional.accuracy(preds, y, task=\"multiclass\", num_classes=CLASSIFICATION_CATEGORIES_COUNT)\n",
    "      acc_total += acc_batch\n",
    "\n",
    "    loss_total /= len(test_dl)\n",
    "    acc_total /= len(test_dl)\n",
    "  return (loss_total, acc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the whole population simultaneously, populating the individuals' fitness value parameters:\n",
    "- `train_losses` ... a dictionary filled with the train loss function results for each batch (independent of how often we test, because it comes for free)\n",
    "- `test_losses` ... same as above but evaluating on test data instead, and only whenever we choose to test (obviously; this is not free)\n",
    "- `accs` ... a dictionary with same keys as `test_losses`, filled with the fraction of correct model predictions by total number of predictions\n",
    "- `acc` ... a single number - the most recent accuracy (defined similarly as `accs`)\n",
    "- `running_acc` ... a single number - the sum of all known accuracies (i.e. at all times where we tested), divided by the time\n",
    "    - here, we exclude the first accuracy because the division is very big in that case, and accuracy only depends mainly on weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def train_and_evaluate_gen(pop: NN_population,\n",
    "                           train_dl,  # <- train dataloader\n",
    "                           test_dl,   # <- test dataloader\n",
    "                           testing_interval = 300,      # <- after how many batches should we test an individual\n",
    "                           testing_data_fraction = 1.0, # <- amount of test_dl to be used (1=100% takes a lot of time)\n",
    "                           training_data_fraction = 1.0,# <- amount of train_dl to be used (1=100% takes a lot of time)\n",
    "                           epochs = 5,\n",
    "                           live_plot = True,\n",
    "                           only_last_plot = False,\n",
    "                           no_plot = False):\n",
    "  start_time = time.perf_counter() # Start timing\n",
    "\n",
    "  # prepare the reduced testing data loader\n",
    "  random_batch_indices = random.sample(range(len(test_dl)), int(len(test_dl) * testing_data_fraction)) # random indices (without replacement)\n",
    "  test_subset = Subset(test_dl.dataset, random_batch_indices)\n",
    "  test_subset_dl = DataLoader(test_subset, batch_size=test_dl.batch_size, shuffle=False, num_workers=test_dl.num_workers)\n",
    "\n",
    "  # prepare the reduced training data loader ()\n",
    "  first_indices = list(range(int(len(train_dl)*train_dl.batch_size * training_data_fraction)))\n",
    "  train_subset = Subset(train_dl.dataset, first_indices)\n",
    "  train_subset_dl = DataLoader(train_subset, batch_size=train_dl.batch_size, shuffle=False, num_workers=train_dl.num_workers)\n",
    "\n",
    "  # re-initialize pop's fitness values:\n",
    "  for ind in pop:\n",
    "    ind.acc, ind.running_acc = 0, 0\n",
    "    ind.train_losses, ind.test_losses, ind.accs = {}, {}, {}\n",
    "  \n",
    "  # train each individual \"simultaneously\" by making the epoch-loop the outer one\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"*** Commencing epoch {epoch+1} / {epochs} for {len(pop)} individuals, one line each. ***\")\n",
    "    for i in range(len(pop)):\n",
    "      for batch, (X, y) in tqdm(enumerate(train_subset_dl),desc=f\"{i+1}. {pop[i].name} [{pop[i].dna.toString()}]\"):\n",
    "        # train the model (update the weights and biases of the NN pop[i])\n",
    "        pop[i].train_losses[f\"e_{epoch}@b_{batch}\"], elapsed_batch_training_time = train_model_one_batch(pop[i], X=X, y=y)\n",
    "        pop[i].elapsed_training_time += elapsed_batch_training_time\n",
    "        if batch % testing_interval == 0: \n",
    "          # test the model and store the results\n",
    "          pop[i].test_losses[f\"e_{epoch}@b_{batch}\"], pop[i].accs[f\"e_{epoch}@b_{batch}\"] = test_model(pop[i], test_dl=test_subset_dl)\n",
    "          if batch != 0: # don't use the start/benchmark test as this depends mostly on luck of weight initialization\n",
    "            pop[i].running_acc += pop[i].accs[f\"e_{epoch}@b_{batch}\"] / pop[i].elapsed_training_time\n",
    "          if live_plot and not only_last_plot and not no_plot:\n",
    "            clear_output(wait=True)\n",
    "            pop.plot_accs(time.perf_counter() - start_time)\n",
    "      pop[i].test_losses[f\"e_{epoch}@end\"], pop[i].accs[f\"e_{epoch}@end\"] = test_model(pop[i], test_dl=test_dl) # latest precise values\n",
    "      pop[i].acc = pop[i].accs[f\"e_{epoch}@end\"] # store the very last known accuracy\n",
    "      if not live_plot and not only_last_plot and not no_plot:\n",
    "        clear_output(wait=True)\n",
    "        pop.plot_accs(time.perf_counter() - start_time)\n",
    "    # here we could select directly, i.e. before the whole train_dl over max_epochs no. of iterations has been trained\n",
    "\n",
    "  if not no_plot and not only_last_plot:\n",
    "    clear_output(wait=True)\n",
    "    pop.plot_accs(time.perf_counter() - start_time)\n",
    "  elif not no_plot and only_last_plot:\n",
    "    pop.plot_accs(time.perf_counter() - start_time)\n",
    "  else:  \n",
    "    print(f\"This took {time.perf_counter() - start_time:.2f}s.\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutate\n",
    "Should be guided by randomness, and not too radical.\n",
    "1. Fix a bound of *radicalities* fixed at 1.\n",
    "2. Each possible operation of mutation needs to come with a *factor of impact* **λ**\n",
    "    - e.g. delete/add 2d block: **λ = 5**\n",
    "    - e.g. learning rate times 1.1: **λ = 0.5**\n",
    "3. Now we spin the wheels for each call of `mutate_individual`:\n",
    "    - random radicality **0 < R < 1**\n",
    "    - random raw likelihood **0 < p < 1** for each possible operation\n",
    "    - execute an operation iff **λ•p < R** where **λ** and **p** correspond to the operation in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "MAX_KERNEL_SIZE = 11\n",
    "class Mutation():\n",
    "    def __init__(self):\n",
    "        self.R = random.random() # <- radicality of this mutation instance\n",
    "\n",
    "    ### factors of impact ###\n",
    "    impact_lr_factor_1_1=.3\n",
    "    impact_lr_factor_10=3\n",
    "    impact_add_neuron_to_2d_block=1\n",
    "    impact_delete_neuron_from_2d_block=1\n",
    "    impact_add_2d_block=5\n",
    "    impact_delete_2d_block=5\n",
    "    impact_increase_kernel=.8\n",
    "    impact_decrease_kernel=.8\n",
    "    impact_change_optimizer=10\n",
    "    impact_change_loss_fn=5\n",
    "\n",
    "    ### possible operations (p_raw = 0 will happen most likely, p_raw = 1 least likely, p_raw = -1 definitely) ###\n",
    "    def lr_factor_1_1(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_lr_factor_1_1 < self.R:\n",
    "            if random.random() > .5:\n",
    "                dna.lr *= 1.1\n",
    "                return \"multiplied lr by 1.1\"\n",
    "            else:\n",
    "                dna.lr /= 1.1\n",
    "                return \"divided lr by 1.1\"\n",
    "    def lr_factor_10(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_lr_factor_10 < self.R:\n",
    "            if random.random() > .5:\n",
    "                dna.lr *= 10\n",
    "                return \"multiplied lr by 10\"\n",
    "            else:\n",
    "                dna.lr /= 10\n",
    "                return \"divided lr by 10\"\n",
    "    def add_neuron_to_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_add_neuron_to_2d_block < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            dna.blocks_2d_gene[layer_nr].out_channels += 1\n",
    "            return f\"added neuron to 2d block no. {layer_nr}\"\n",
    "    def delete_neuron_from_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_delete_neuron_from_2d_block < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            if dna.blocks_2d_gene[layer_nr].out_channels > 1: dna.blocks_2d_gene[layer_nr].out_channels -= 1\n",
    "            return f\"deleted neuron from 2d block no. {layer_nr}\"\n",
    "    def add_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        old_dna = copy.deepcopy(dna)\n",
    "        if p_raw * self.impact_add_2d_block < self.R:\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene) + 1)\n",
    "            input_image_size = IMAGE_WIDTH if layer_nr == 0 else dna.blocks_2d_gene[layer_nr-1].output_image_size\n",
    "            dna.blocks_2d_gene.insert(layer_nr, create_random_2d_block(input_image_size, MAX_KERNEL_SIZE))\n",
    "            # check whether this insertion \"killed\" the entity (and if yes: repair it)\n",
    "            protocol = update_and_check_2d_block_stack(dna.blocks_2d_gene)\n",
    "            return f\"added block @ {layer_nr} ({protocol})\"\n",
    "    def delete_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_delete_2d_block < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            dna.blocks_2d_gene.pop(layer_nr)\n",
    "            # check whether this deletion \"killed\" the entity (and if yes: repair it)\n",
    "            protocol = update_and_check_2d_block_stack(dna.blocks_2d_gene)\n",
    "            return f\"deleted 2d block at {layer_nr} ({protocol})\"\n",
    "    def increase_kernel(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_increase_kernel < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            if random.random() > .5:\n",
    "                # check whether the kernel may be increased\n",
    "                if dna.blocks_2d_gene[layer_nr].conv_kernel_size < dna.blocks_2d_gene[layer_nr].input_image_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].conv_kernel_size += 1\n",
    "                    return f\"conv kernel += 1 of 2d block no. {layer_nr}\"\n",
    "            else:\n",
    "                if dna.blocks_2d_gene[layer_nr].pool_kernel_size < dna.blocks_2d_gene[layer_nr].after_conv_image_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].pool_kernel_size += 1\n",
    "                    return f\"pool kernel +=1 of 2d block no. {layer_nr}\"\n",
    "    def decrease_kernel(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_decrease_kernel < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            if random.random() > .5:\n",
    "                if dna.blocks_2d_gene[layer_nr].conv_kernel_size > 1 and dna.blocks_2d_gene[layer_nr].conv_padding*2 < dna.blocks_2d_gene[layer_nr].conv_kernel_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].conv_kernel_size -= 1\n",
    "                    return f\"conv kernel -= 1 of 2d block no. {layer_nr}\"\n",
    "            else:\n",
    "                if dna.blocks_2d_gene[layer_nr].pool_kernel_size > 1 and dna.blocks_2d_gene[layer_nr].pool_padding*2 < dna.blocks_2d_gene[layer_nr].pool_kernel_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].pool_kernel_size -= 1\n",
    "                    return f\"pool kernel -= 1 of 2d block no. {layer_nr}\"\n",
    "    def change_optimizer(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_change_optimizer < self.R:\n",
    "            optimizer_index = random.randrange(0, len(NN_dna.optimizer_dict))\n",
    "            if optimizer_index == dna.optimizer_gene: return None\n",
    "            dna.optimizer_gene = optimizer_index\n",
    "            return f\"changed optimizer to {NN_dna.optimizer_dict[optimizer_index]}\"\n",
    "    def change_loss_fn(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_change_loss_fn < self.R:\n",
    "            loss_fn_index = random.randrange(0, len(NN_dna.loss_fn_dict))\n",
    "            if loss_fn_index == dna.loss_fn_gene: return None\n",
    "            dna.loss_fn_gene = loss_fn_index\n",
    "            return f\"changed loss function to {NN_dna.loss_fn_dict[loss_fn_index]['name']}\"\n",
    "\n",
    "def mutant_from_dna(dna_parent: NN_dna, print_actions: bool = True, mutant_name = \"NN\", device = device) -> NN_individual:\n",
    "    # clone the dna (to not change the parent's dna)\n",
    "    dna_mutant = copy.deepcopy(dna_parent) \n",
    "\n",
    "    # create a test batch\n",
    "    test_batch = torch.rand(MINI_BATCH_SIZE, COLOUR_CHANNEL_COUNT, IMAGE_WIDTH, IMAGE_HEIGHT).to(device)\n",
    "\n",
    "    # create a mutation instance (this produces a radicality R)\n",
    "    m = Mutation()\n",
    "    if print_actions: print(f\"Radicality in creation of '{mutant_name}': {m.R:.2g}\")\n",
    "\n",
    "    # Dynamically loop over all functions of the Mutation class\n",
    "    for method_name in dir(m): # Loop through all attributes of the class\n",
    "        if callable(getattr(m, method_name)) and not method_name.startswith(\"__\"): # Filter to only functions (ignoring private methods and attributes)\n",
    "            p_raw = random.random() # raw likelihood\n",
    "            operation = getattr(m, method_name)\n",
    "            mutation_healthy, deaths = False, 0\n",
    "            while not mutation_healthy:\n",
    "                dna_before_mutation = copy.deepcopy(dna_mutant) # clone the dna (to not change the parent's dna)\n",
    "                effect = operation(dna_mutant, p_raw) # execute the operation\n",
    "                mutant = NN_individual(dna_mutant, mutant_name, device)\n",
    "                try: \n",
    "                    mutant(test_batch)\n",
    "                    mutation_healthy = True\n",
    "                except Exception as e:\n",
    "                    if print_actions: print(f\"\\tLETHAL MUTATION ({effect}) (e: '{e}')\")\n",
    "                    dna_mutant = dna_before_mutation # restore to the state before lethal operation\n",
    "                    deaths += 1\n",
    "                    if deaths > 10: raise Exception(\"Too many deaths.\")\n",
    "            if effect != None and print_actions: print(f\"- {effect}\")\n",
    "    return mutant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP Run\n",
    "1. Create a random population of given POP_SIZE\n",
    "2. `train_and_evaluate_gen` for one epoch\n",
    "3. choose the half of `NN_individual`s that has the best `running_acc`\n",
    "4. mutate each chosen NN **twice**\n",
    "5. continue at *2.* unless you are already at gen FINAL_GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_evolution_df(gens: list[NN_population]):\n",
    "    inds = sorted([gens[i][j] for i in range(len(gens)) for j in range(len(gens[i]))], key=lambda ind: ind.name) # all individuals of all generations, sorted by name\n",
    "    rows = []\n",
    "    for ind in inds:\n",
    "        rows.append([ind.name, '['+ind.dna.toString()+']', ind.running_acc, ind.acc.item()*100, f\"{ind.elapsed_training_time:.2f}s\"])\n",
    "    df = pd.DataFrame(rows, columns=['Name','DNA','running_acc','acc','training time'])#.sort_values(by='running_acc',ascending=False)\n",
    "    cmap_bad_good = LinearSegmentedColormap.from_list(\"custom_map\", [\"darkred\", \"white\", \"green\"])\n",
    "    df_styled = df.style.background_gradient(cmap_bad_good).format({'running_acc':\"{:.3g}\",'acc':\"{:.1f}\"}).set_properties(**{'text-align': 'left'})\n",
    "\n",
    "    return df_styled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generations(gens):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Store positions for each individual (to create branches)\n",
    "    positions = {}  \n",
    "    branch_lines = []\n",
    "    colors = {}  # To assign a unique color for each initial individual\n",
    "    initial_names = [ind.name for ind in gens[0]]  # Get the names of initial individuals\n",
    "\n",
    "    # Generate a colormap with distinct colors for each initial individual\n",
    "    color_map = plt.colormaps.get_cmap('tab10')  # Get a colormap with enough unique colors\n",
    "\n",
    "    for i, name in enumerate(initial_names):\n",
    "        colors[name] = color_map(i)\n",
    "\n",
    "    # Plot each generation\n",
    "    for i, gen in enumerate(gens):\n",
    "        for ind in gen:\n",
    "            name = ind.name\n",
    "            # Find the root ancestor (initial individual) for this individual\n",
    "            root_name = name.split('.')[0]\n",
    "\n",
    "            # Determine parent (if exists)\n",
    "            parent_name = name.rsplit('.', 1)[0] if '.' in name else None\n",
    "\n",
    "            # Store the current generation and its value for the plot\n",
    "            x = i  # Generation is the x-axis\n",
    "            y = ind.acc.cpu()*100  # Value is the y-axis\n",
    "            \n",
    "            # Add the individual to the positions dict\n",
    "            if name not in positions:\n",
    "                positions[name] = (x, y)\n",
    "\n",
    "            # If the individual has a parent, create a branch line\n",
    "            if parent_name and parent_name in positions:\n",
    "                parent_x, parent_y = positions[parent_name]\n",
    "                branch_lines.append([(parent_x, parent_y), (x, y), root_name])\n",
    "\n",
    "            # Plot the individual as a point (using root_name color)\n",
    "            ax.scatter(x, y, color=colors[root_name])\n",
    "\n",
    "    # Draw the branch lines\n",
    "    for branch in branch_lines:\n",
    "        (x0, y0), (x1, y1), root_name = branch\n",
    "        ax.plot([x0, x1], [y0, y1], color=colors[root_name])\n",
    "\n",
    "    # Create a legend for the initial individuals\n",
    "    handles = [plt.Line2D([0], [0], color=colors[name], lw=2, label=name) for name in initial_names]\n",
    "    ax.legend(handles=handles, title='Lineage', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Set labels\n",
    "    ax.set_xlabel(\"Generations\")\n",
    "    ax.set_ylabel(\"Accuracy [%]\")\n",
    "    ax.set_title(\"NN Individual Evolution Across Generations\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Gen. 1 / 5 *****\n",
      "*** Commencing epoch 1 / 1 for 10 individuals, one line each. ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1. NN0 [(5,8) SGD@λ1μ0 & L1 @ 0.1]: 938it [00:07, 125.74it/s]\n",
      "2. NN1 [(15) adam & Hub@δ=.1 @ 0.1]: 938it [00:07, 131.17it/s]\n",
      "3. NN2 [(10) SGD@λ-1μ-1 & L1 @ 0.1]: 938it [00:09, 103.64it/s]\n",
      "4. NN3 [(3,13,9) SGD@λ0μ0 & Hub@δ=10 @ 0.1]: 938it [00:10, 87.88it/s] \n",
      "5. NN4 [(15,10,13,15) SGD@λ1μ1 & L1 @ 0.1]: 938it [00:10, 91.06it/s] \n",
      "6. NN5 [(15,4,12,4) SGD@λ1μ1 & Hub@δ=10 @ 0.1]: 938it [00:09, 94.42it/s] \n",
      "7. NN6 [(7,3,14,9,6) SGD@λ-1μ-1 & L1 @ 0.1]: 938it [00:10, 85.44it/s]\n",
      "8. NN7 [(4) SGD@λ-1μ-1 & L1 @ 0.1]: 938it [00:08, 110.33it/s]\n",
      "9. NN8 [(7,10,4) SGD@λ-1μ0 & Hub@δ=10 @ 0.1]: 938it [00:09, 96.25it/s] \n",
      "10. NN9 [(5,12) SGD@λ1μ1 & Hub@δ=.1 @ 0.1]: 938it [00:09, 102.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 114.73s.\n",
      "Radicality in creation of 'NN9.a': 0.45\n",
      "- deleted neuron from 2d block no. 1\n",
      "- conv kernel += 1 of 2d block no. 1\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN9.b': 0.36\n",
      "- added neuron to 2d block no. 0\n",
      "- changed loss function to Hub@δ=10\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN1.a': 0.77\n",
      "- added neuron to 2d block no. 0\n",
      "- conv kernel -= 1 of 2d block no. 0\n",
      "- deleted 2d block at 0 (no block in the stack)\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN1.b': 0.21\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN2.a': 0.41\n",
      "- added block @ 0 (block[1]'s input_image_size was set to block[0]'s output_image_size (29), )\n",
      "- added neuron to 2d block no. 1\n",
      "- deleted neuron from 2d block no. 1\n",
      "- pool kernel +=1 of 2d block no. 0\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN2.b': 0.44\n",
      "- added neuron to 2d block no. 0\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN5.a': 0.29\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN5.b': 0.6\n",
      "\tLETHAL MUTATION (added block @ 1 (block[2]'s input_image_size was set to block[1]'s output_image_size (3), block[2]'s conv_kernel_size was decreased to input_image_size (3), block[2]'s pool_kernel_size was decreased to after_conv_size (0), )) (e: 'kernel size should be greater than zero, but got kH: 0 kW: 0')\n",
      "- added block @ 3 ()\n",
      "- conv kernel -= 1 of 2d block no. 0\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN0.a': 0.76\n",
      "- added neuron to 2d block no. 0\n",
      "- deleted neuron from 2d block no. 0\n",
      "- pool kernel +=1 of 2d block no. 1\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN0.b': 0.87\n",
      "\tLETHAL MUTATION (added block @ 0 (block[1]'s input_image_size was set to block[0]'s output_image_size (6), block[1]'s conv_kernel_size was decreased to input_image_size (6), block[1]'s pool_kernel_size was decreased to after_conv_size (0), )) (e: 'kernel size should be greater than zero, but got kH: 0 kW: 0')\n",
      "- added block @ 2 ()\n",
      "- added neuron to 2d block no. 1\n",
      "- conv kernel += 1 of 2d block no. 2\n",
      "- multiplied lr by 1.1\n",
      "***** Gen. 2 / 5 *****\n",
      "*** Commencing epoch 1 / 1 for 10 individuals, one line each. ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1. NN9.a [(5,11) SGD@λ1μ1 & Hub@δ=.1 @ 0.11]: 938it [00:09, 96.41it/s] \n",
      "2. NN9.b [(6,12) SGD@λ1μ1 & Hub@δ=10 @ 0.11]: 938it [00:09, 101.99it/s]\n",
      "3. NN1.a [) adam & Hub@δ=.1 @ 0.091]: 938it [00:07, 123.34it/s]\n",
      "4. NN1.b [(15) adam & Hub@δ=.1 @ 0.091]: 938it [00:08, 108.59it/s]\n",
      "5. NN2.a [(4,10) SGD@λ-1μ-1 & L1 @ 0.091]: 938it [00:09, 100.79it/s]\n",
      "6. NN2.b [(11) SGD@λ-1μ-1 & L1 @ 0.091]: 938it [00:08, 108.77it/s]\n",
      "7. NN5.a [(15,4,12,4) SGD@λ1μ1 & Hub@δ=10 @ 0.11]: 938it [00:09, 94.26it/s] \n",
      "8. NN5.b [(15,4,12,3,4) SGD@λ1μ1 & Hub@δ=10 @ 0.091]: 938it [00:10, 89.08it/s]\n",
      "9. NN0.a [(5,8) SGD@λ1μ0 & L1 @ 0.091]: 938it [00:09, 104.13it/s]\n",
      "10. NN0.b [(5,9,5) SGD@λ1μ0 & L1 @ 0.11]: 938it [00:09, 98.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 113.61s.\n",
      "Radicality in creation of 'NN1.a.a': 0.18\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN1.a.b': 0.8\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN2.b.a': 0.51\n",
      "- conv kernel += 1 of 2d block no. 0\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN2.b.b': 0.28\n",
      "- deleted neuron from 2d block no. 0\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN1.b.a': 0.2\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN1.b.b': 0.31\n",
      "- deleted neuron from 2d block no. 0\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN0.a.a': 0.57\n",
      "\tLETHAL MUTATION (added block @ 2 ()) (e: 'Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size')\n",
      "- added block @ 2 ()\n",
      "- added neuron to 2d block no. 0\n",
      "- pool kernel +=1 of 2d block no. 0\n",
      "- multiplied lr by 10\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN0.a.b': 0.75\n",
      "- added neuron to 2d block no. 0\n",
      "- divided lr by 1.1\n",
      "Radicality in creation of 'NN2.a.a': 0.64\n",
      "- added neuron to 2d block no. 0\n",
      "- deleted 2d block at 0 (block[0]'s input_image_size was set to IMAGE_HEIGHT (28), )\n",
      "- conv kernel += 1 of 2d block no. 0\n",
      "- multiplied lr by 1.1\n",
      "Radicality in creation of 'NN2.a.b': 0.54\n",
      "- added neuron to 2d block no. 1\n",
      "- multiplied lr by 1.1\n",
      "***** Gen. 3 / 5 *****\n",
      "*** Commencing epoch 1 / 1 for 10 individuals, one line each. ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1. NN1.a.a [) adam & Hub@δ=.1 @ 0.1]: 938it [00:07, 119.43it/s]\n",
      "2. NN1.a.b [) adam & Hub@δ=.1 @ 0.083]: 938it [00:07, 126.08it/s]\n",
      "3. NN2.b.a [(11) SGD@λ-1μ-1 & L1 @ 0.1]: 938it [00:08, 110.47it/s]\n",
      "4. NN2.b.b [(10) SGD@λ-1μ-1 & L1 @ 0.1]: 938it [00:08, 110.07it/s]\n",
      "5. NN1.b.a [(15) adam & Hub@δ=.1 @ 0.1]: 938it [00:08, 111.06it/s]\n",
      "6. NN1.b.b [(14) adam & Hub@δ=.1 @ 0.1]: 938it [00:08, 112.73it/s]\n",
      "7. NN0.a.a [(6,8,9) SGD@λ1μ0 & L1 @ 1]: 938it [00:09, 99.48it/s] \n",
      "8. NN0.a.b [(6,8) SGD@λ1μ0 & L1 @ 0.083]: 938it [00:09, 98.66it/s] \n",
      "9. NN2.a.a [(10) SGD@λ-1μ-1 & L1 @ 0.1]: 466it [00:06, 101.92it/s]"
     ]
    }
   ],
   "source": [
    "POP_SIZE = 10\n",
    "FINAL_GEN = 5\n",
    "MAX_2D_BLOCK_COUNT = 5\n",
    "MAX_KERNEL_SIZE = 11\n",
    "\n",
    "testing_interval = 100\n",
    "epochs = 1\n",
    "testing_data_fraction = .5\n",
    "train_data_fraction = .5\n",
    "derivatives = ['a','b']\n",
    "\n",
    "gens = []\n",
    "\n",
    "gens.append(create_random_population(POP_SIZE, max_2d_block_count=MAX_2D_BLOCK_COUNT, max_kernel_size=MAX_KERNEL_SIZE, name_prefix=\"NN\", device=device, print_summary=False))\n",
    "while(True):\n",
    "    print(f\"***** Gen. {len(gens)} / {FINAL_GEN} *****\")\n",
    "    train_and_evaluate_gen(gens[-1], train_dl_f_mnist, test_dl_f_mnist, testing_interval, testing_data_fraction, training_data_fraction=train_data_fraction, epochs=epochs, live_plot=False, only_last_plot=True, no_plot=True)\n",
    "    if len(gens) == FINAL_GEN: break\n",
    "    survivors = sorted(gens[-1], key=lambda ind: ind.running_acc, reverse=True)[:len(gens[-1]) // 2] # pick the top half (w.r.t. running_acc)\n",
    "    gens.append(NN_population([mutant_from_dna(ind.dna, mutant_name=ind.name+\".\"+v, device=device) for ind in survivors for v in derivatives])) # mutate each survivor (as often as len(derivatives))\n",
    "\n",
    "print(f\"******* SUMMARY *******\")\n",
    "NN_population([gens[i][j] for i in range(len(gens)) for j in range(len(gens[i]))]).plot_accs()\n",
    "plot_generations(gens)\n",
    "return_evolution_df(gens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- change the mutation for the lr by using a Gauss distribution weighed by the p_raw and radicality\n",
    "- increase the space of possible mutations / initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN0.a.a.a.a: (3,3,6,) SGD & CrossEn @ 0.15\n",
      "out_channels = 3 <-- (28 x 28)\n",
      "conv_2d (kernel, stride, padding) =\t(5, 3, 0) --> (8 x 8)\n",
      "max_pool_2d (kernel, stride, padding) =\t(2, 1, 1) --> (9 x 9)\n",
      "out_channels = 3 <-- (9 x 9)\n",
      "conv_2d (kernel, stride, padding) =\t(4, 4, 1) --> (2 x 2)\n",
      "max_pool_2d (kernel, stride, padding) =\t(1, 1, 0) --> (2 x 2)\n",
      "out_channels = 6 <-- (2 x 2)\n",
      "conv_2d (kernel, stride, padding) =\t(2, 1, 0) --> (1 x 1)\n",
      "max_pool_2d (kernel, stride, padding) =\t(1, 1, 0) --> (1 x 1)\n",
      "--------------------\n",
      "NN0.b.a: (3,3,6,) SGD & CrossEn @ 0.1\n",
      "out_channels = 3 <-- (28 x 28)\n",
      "conv_2d (kernel, stride, padding) =\t(5, 3, 0) --> (8 x 8)\n",
      "max_pool_2d (kernel, stride, padding) =\t(2, 1, 1) --> (9 x 9)\n",
      "out_channels = 3 <-- (9 x 9)\n",
      "conv_2d (kernel, stride, padding) =\t(4, 4, 1) --> (2 x 2)\n",
      "max_pool_2d (kernel, stride, padding) =\t(1, 1, 0) --> (2 x 2)\n",
      "out_channels = 6 <-- (2 x 2)\n",
      "conv_2d (kernel, stride, padding) =\t(2, 1, 0) --> (1 x 1)\n",
      "max_pool_2d (kernel, stride, padding) =\t(1, 1, 0) --> (1 x 1)\n"
     ]
    }
   ],
   "source": [
    "ind = [ind for ind in gens[4].individuals if ind.name==\"NN0.a.a.a.a\"][0]\n",
    "ind2 = [ind for ind in gens[2].individuals if ind.name==\"NN0.b.a\"][0]\n",
    "#ind3 = [ind for ind in pop.individuals if ind.name==\"NN3.a\"][0]\n",
    "print(f\"{ind.name}: {ind.dna.toString()}\")\n",
    "for block in ind.dna.blocks_2d_gene:\n",
    "    print(f\"{block.toString()}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"{ind2.name}: {ind2.dna.toString()}\")\n",
    "for block in ind2.dna.blocks_2d_gene:\n",
    "    print(f\"{block.toString()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
