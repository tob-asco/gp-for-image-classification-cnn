{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121 running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"{torch.__version__} running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Run for Image Classification\n",
    "Your problem needs to fulfill the following criteria.\n",
    "1. It is an image classification problem.\n",
    "2. You supply marked training images and marked validation images.\n",
    "\n",
    "Within those, the run is flexible and adapts itself to your problem.\n",
    "Now, please describe your images and problem by setting those global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28 # <-- number of width-pixels\n",
    "IMAGE_HEIGHT = 28 # <-- number of height-pixels\n",
    "COLOUR_CHANNEL_COUNT = 1 # <-- RGB images would have 3\n",
    "CLASSIFICATION_CATEGORIES_COUNT = 10 # <-- the amount of possible categories of which each image shall be marked with one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Example Data\n",
    "To check the code, we prepare example data: Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data (and not testing data)\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
    "print(f\"train_data.classes = {train_data.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dl_f_mnist.batch_size = 32\n",
      "len(next(iter(train_dl_f_mnist))) = 2\n",
      "next(iter(train_dl_f_mnist))[0].shape = torch.Size([32, 1, 28, 28])\n",
      "len(train_dl_f_mnist) = 1875, len(test_dl_f_mnist) = 313\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "MINI_BATCH_SIZE = 32 # constant for now\n",
    "\n",
    "# Turn datasets into iterables (batches), shuffeling train data every epoch (test data not)\n",
    "train_dl_f_mnist = DataLoader(train_data, batch_size=MINI_BATCH_SIZE, shuffle=True)\n",
    "test_dl_f_mnist = DataLoader(test_data, batch_size=MINI_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"train_dl_f_mnist.batch_size = {train_dl_f_mnist.batch_size}\") \n",
    "print(f\"len(next(iter(train_dl_f_mnist))) = {len(next(iter(train_dl_f_mnist)))}\") \n",
    "print(f\"next(iter(train_dl_f_mnist))[0].shape = {next(iter(train_dl_f_mnist))[0].shape}\") \n",
    "print(f\"len(train_dl_f_mnist) = {len(train_dl_f_mnist)}, len(test_dl_f_mnist) = {len(test_dl_f_mnist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 2 # index in the batch, 0 .. 31\n",
    "train_features_batch, train_labels_batch = next(iter(train_dl_f_mnist))\n",
    "print(f\"Image shape: {train_features_batch[image_index].shape}\")\n",
    "plt.imshow(train_features_batch[image_index].squeeze(), cmap=\"gray\") # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(str(train_labels_batch[image_index].item())+\" i.e. \"+train_data.classes[train_labels_batch[image_index].item()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Individuals' Class\n",
    "- The Hyperparamters should be passed to the constructor in a way that is both convenient for GP and for PyTorch.\n",
    "- I think I want to define a class for one `nn.Sequential` 2d-block\n",
    "    - All possible instances should be concatenable with all possible instances\n",
    "- Then, an individual is built from the concatenation of many such blocks, plus data preparation and final f.c. layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' superseded later in the notebook...\\nclass Sequential_block_2d(nn.Module):\\n    def __init__(self, \\n                 out_channels: int, # the number of output neurons after the full block\\n                 in_channels: int = 1, # should not be set here, but is set in Individual\\'s __init__()\\n                 conv_kernel_size: int = 3,\\n                 conv_stride: int = 1,\\n                 conv_padding: int = 1,\\n                 pool_kernel_size: int = 2,\\n                 pool_stride: int = 2,\\n                 pool_padding: int = 0):\\n        super().__init__()\\n        self.in_channels = in_channels\\n        self.out_channels = out_channels\\n        self.block = nn.Sequential(\\n            nn.Conv2d(in_channels=in_channels,\\n                      out_channels=out_channels,\\n                      kernel_size=conv_kernel_size,\\n                      stride=conv_stride,\\n                      padding=conv_padding),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=pool_kernel_size,\\n                         stride=pool_stride,\\n                         padding=pool_padding)\\n        )\\n    def forward(self, x):\\n        return self.block(x)\\n\\n# generate some testing blocks (TODO: write actual unit tests)\\ntestBlock1 = Sequential_block_2d(in_channels=1,out_channels=5)\\ntestBlock2 = Sequential_block_2d(in_channels=5,out_channels=3)\\ntorch.manual_seed(42)\\ntestX = torch.randn(COLOUR_CHANNEL_COUNT,IMAGE_WIDTH,IMAGE_HEIGHT)\\nprint(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\\nprint(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded later in the notebook...\n",
    "class Sequential_block_2d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_channels: int, # the number of output neurons after the full block\n",
    "                 in_channels: int = 1, # should not be set here, but is set in Individual's __init__()\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=conv_kernel_size,\n",
    "                      stride=conv_stride,\n",
    "                      padding=conv_padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size,\n",
    "                         stride=pool_stride,\n",
    "                         padding=pool_padding)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# generate some testing blocks (TODO: write actual unit tests)\n",
    "testBlock1 = Sequential_block_2d(in_channels=1,out_channels=5)\n",
    "testBlock2 = Sequential_block_2d(in_channels=5,out_channels=3)\n",
    "torch.manual_seed(42)\n",
    "testX = torch.randn(COLOUR_CHANNEL_COUNT,IMAGE_WIDTH,IMAGE_HEIGHT)\n",
    "print(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\n",
    "print(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' superseded later in the notebook...\\n# class for image classification individuals\\nclass NN_individual(nn.Module):\\n    def __init__(self, blocks_2d: list[Sequential_block_2d],\\n                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\\n                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\\n                 fin_res: int = 5): # output dimension of the final max pooling \\n                                    # producing size (fin_res * fin_res)\\n        super().__init__()\\n        self.blocks_2d = blocks_2d\\n        # add a final max pool\\n        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\\n        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\\n        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\\n        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\\n                      out_features = out_dimensions)\\n    def forward(self, x):\\n        for i in range(len(self.blocks_2d)):\\n            x = self.blocks_2d[i](x)\\n        x = self.last_max_pool_adaptive(x)\\n        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\\n        x = self.flatten(x)\\n        print(f\"flatten output shape is {x.shape}\")\\n        x = self.lin(x)\\n        print(f\"lin output shape is {x.shape}\")\\n        return x\\n    \\ntestIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\\ntestIndividual(testX)\\n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded later in the notebook...\n",
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, blocks_2d: list[Sequential_block_2d],\n",
    "                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\n",
    "                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\n",
    "                 fin_res: int = 5): # output dimension of the final max pooling \n",
    "                                    # producing size (fin_res * fin_res)\n",
    "        super().__init__()\n",
    "        self.blocks_2d = blocks_2d\n",
    "        # add a final max pool\n",
    "        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\n",
    "        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\n",
    "                      out_features = out_dimensions)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.last_max_pool_adaptive(x)\n",
    "        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lin(x)\n",
    "        print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\n",
    "testIndividual(testX)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_channels of the first needs to match in_channels of the second!\n",
      "in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\n"
     ]
    }
   ],
   "source": [
    "# this way, adjacent genes (= 2d_blocks) need to have the correct dimensions\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=1,out_channels=2), Sequential_block_2d(in_channels=3, out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"out_channels of the first needs to match in_channels of the second!\")\n",
    "\n",
    "# but there's more redundancy:\\the first gene needs to have the same number of in_channels as there are colour channels\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=COLOUR_CHANNEL_COUNT + 1,out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brainstorm on How to Encode Individuals\n",
    "We need to talk about this right now because we want to adapt our `NN_individual.blocks_2d` definition according to it.\n",
    "The options are:\n",
    "1. We specify `Sequential_block_2d.in_channels` and `~.out_channels` separately for each individual and only allow concatenation if the criteria are met. This is probably not super clever...\n",
    "2. Genotype-closure: The parameters that are adapted through GP will never leave the space of syntacticly correct indivuals\n",
    "    - The first gene is not allowed to choose `~.in_channels`, it must match `COLOUR_CHANNEL_COUNT`\n",
    "    - Every gene but the first is not allowed to choose `~.in_channels`, it must match `~.out_channels` of the prior gene\n",
    "    - How will this change the gene class `Sequential_block_2d`?\n",
    "        - Set `Sequential_block_2d.in_channels` only programatically, in `NN_individual.__init__()`\n",
    "        - Don't even let the genes inherit from `nn.Module`, only the individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the genetic information of one 2d block\n",
    "class Gene_2d_block:\n",
    "    def __init__(self,\n",
    "                 out_channels: int,\n",
    "                 in_channels: int = None, # set in the individual's constructor\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_stride = conv_stride\n",
    "        self.conv_padding = conv_padding\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.pool_padding = pool_padding\n",
    "\n",
    "    def toString(self, tab_count: int = 0):\n",
    "        indentation = \"\"\n",
    "        for tab in range(tab_count): indentation += f\"\\t\"\n",
    "        return f\"{indentation}out_channels = {self.out_channels}\\n\"+\\\n",
    "        f\"{indentation}conv_2d (kernel, stride, padding) =\\t({self.conv_kernel_size}, {self.conv_stride}, {self.conv_padding})\\n\"+\\\n",
    "        f\"{indentation}max_pool_2d (kernel, stride, padding) =\\t({self.pool_kernel_size}, {self.pool_stride}, {self.pool_padding})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this is supserseded below...\\n# class for image classification individuals\\nclass NN_individual(nn.Module):\\n    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \\n        super().__init__()\\n        self.name=name # a name for easier tracking inside a GP run\\n        # build the full sequential from the gene information (genes_2d_block)\\n        self.blocks_2d = nn.Sequential()\\n        # the first 2d_block needs to have as many in_channels as there are colour channels\\n        # the others need to have as in_channels the number of out_channels from the previous block\\n        for i in range(len(genes_2d_block)):\\n            if i == 0:\\n                in_channels = COLOUR_CHANNEL_COUNT\\n            else:\\n                in_channels = genes_2d_block[i-1].out_channels\\n            self.blocks_2d.append(nn.Sequential(\\n                nn.Conv2d(in_channels=in_channels,\\n                    out_channels=genes_2d_block[i].out_channels,\\n                    kernel_size=genes_2d_block[i].conv_kernel_size,\\n                    stride=genes_2d_block[i].conv_stride,\\n                    padding=genes_2d_block[i].conv_padding),\\n                nn.ReLU(),\\n                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\\n                    stride=genes_2d_block[i].pool_stride,\\n                    padding=genes_2d_block[i].pool_padding)))\\n        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\\n        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\\n    def forward(self, x):\\n        for i in range(len(self.blocks_2d)):\\n            x = self.blocks_2d[i](x)\\n        x = self.flatten(x)\\n        #print(f\"flatten output shape is {x.shape}\")\\n        x = self.lazyLin(x)\\n        #print(f\"lin output shape is {x.shape}\")\\n        return x\\n    \\ntestIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\\ntestIndividual, testIndividual(testX)\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' this is supserseded below...\n",
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \n",
    "        super().__init__()\n",
    "        self.name=name # a name for easier tracking inside a GP run\n",
    "        # build the full sequential from the gene information (genes_2d_block)\n",
    "        self.blocks_2d = nn.Sequential()\n",
    "        # the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "        # the others need to have as in_channels the number of out_channels from the previous block\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            self.blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lazyLin(x)\n",
    "        #print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Code\n",
    "Now we need to decide the genetic stuff.\n",
    "1. Initial Population\n",
    "2. Fitness Measure\n",
    "3. Selection\n",
    "4. Genetic Operators\n",
    "    - Cloning or Crossover\n",
    "    - Mutation\n",
    "    \n",
    "### Hyperparameter-landscape is vast. Here's a list:\n",
    "- Net architecture\n",
    "    - kind of layers, number of layers\n",
    "        - for convolution/pooling: kernel size, stride, padding, (dilation) **[implemented]**\n",
    "    - number of neurons per layer\n",
    "    - activation function for each layer\n",
    "- cost function\n",
    "    - base term (e.g. square cost, log-likelihood, cross-entropy, ect.)\n",
    "    - toppings \n",
    "        - regularization of weights (L2, L1, dropout, etc.)\n",
    "- weights and biases optimization technique (= optimizer)\n",
    "    - SGD (= stochastic gradient descent)\n",
    "    - Hessian technique, i.e. momentum-based descent\n",
    "    - PyTorch's various other (e.g. *Adam* optimizer)\n",
    "- learning parameters\n",
    "    - η ... learning rate\n",
    "        - constant, or epoch-dependent, or accuracy-dependent, or a mix\n",
    "    - \\# of epochs\n",
    "        - constant, or early stopping\n",
    "    - (`mini_batch_size` - this one might be canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Within-One-Gen-Constant Hyperparameters in `NN_individual`\n",
    "We now bake:\n",
    "- the individual-specific, hyperparameters (that don't change within one gen)\n",
    "- the fitness dictionaries\n",
    "\n",
    "into parameters of the `NN_individual` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NN_individual(\n",
       "   (blocks_2d): Sequential()\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (lazyLin): Linear(in_features=784, out_features=10, bias=True)\n",
       "   (loss_fn): CrossEntropyLoss()\n",
       " ),\n",
       " tensor([[ 0.4219,  0.0387,  0.6340,  0.2156,  0.1363,  0.3297,  0.0587,  0.6220,\n",
       "           0.4417, -0.9403]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, \n",
    "                 genes_2d_block: list[Gene_2d_block] = [],  # <- genes for the 2d convolution blocks\n",
    "                 name: str = \"nn0\",                         # <- an inside-population-unique name\n",
    "                 optimizer_gene: str = \"sgd\",               # <- gene for the optimizer\n",
    "                 lr = .1,                                   # <- learning rate\n",
    "                 loss_fn = nn.CrossEntropyLoss(),           # <- loss function\n",
    "                 device = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.blocks_2d = self.__genotype_phenotype_mapping_2d_blocks__(genes_2d_block)\n",
    "        #self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default is: start_dim = 1\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1) # default is: start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "        self.name = name\n",
    "        self.optimizer = self.__genotype_phenotype_mapping_optimizer__(optimizer_gene, lr)\n",
    "        self.lr = lr\n",
    "        self.loss_fn = loss_fn\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.acc = 0\n",
    "        self.running_acc = 0\n",
    "        self.train_losses = {}\n",
    "        self.test_losses = {}\n",
    "        self.accs = {}\n",
    "        self.elapsed_training_time = 0\n",
    "        \n",
    "    def __genotype_phenotype_mapping_2d_blocks__(self, genes_2d_block: list[Gene_2d_block]):\n",
    "        ''' build 'n return the full sequential from the gene information (genes_2d_block) \n",
    "            the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "            the others need to have as in_channels the number of out_channels from the previous block\n",
    "            there's a nn.Module (Lazy*) that automatically infers the number of in_channels - not used here '''\n",
    "        blocks_2d = nn.Sequential()\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        return blocks_2d\n",
    "    \n",
    "    def __genotype_phenotype_mapping_optimizer__(self, optimizer_gene: str, lr: float):\n",
    "        if optimizer_gene.lower() == \"sgd\":\n",
    "            return torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        return torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lazyLin(x)\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual()#genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize, Visualize, Visualize\n",
    "Create a class called population of which an instance acts as an array of `NN_individual`s with extra functionality that regards the whole population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created by Chat\n",
    "class NN_population:\n",
    "    def __init__(self, individuals: list[NN_individual]):\n",
    "        self.individuals = individuals  # This will hold instances of NN_individual\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.individuals[index]  # Allows pop[i] access\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.individuals)  # Allows len(pop)\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self.individuals[index] = value  # Allows pop[i] = value\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.individuals)  # Allows iteration over pop\n",
    "    \n",
    "    def plot_accs(self, elapsed_time = 0):\n",
    "        plt.figure(figsize=(15, 6))  # Set the figure size\n",
    "        for ind in self.individuals:\n",
    "            x = list(ind.accs.keys())  # Extract the epoch/batch labels (x-axis)\n",
    "            y = [float(val.cpu().item()*100) for val in ind.accs.values()]  # Convert tensors to floats\n",
    "            # Plot each individual's accuracies\n",
    "            plt.plot(x, y, marker='o', linestyle='-', label=f\"{ind.name} ({ind.elapsed_training_time:.1f}s, running acc {ind.running_acc:.2f})\")\n",
    "\n",
    "            #plt.annotate( # Add a label to the right-most data point\n",
    "            #    f\"{ind.elapsed_training_time:.1f}s\", # The text to display\n",
    "            #    xy=(x[-1], y[-1]),           # Coordinates of the right-most point\n",
    "            #    xytext=(3, -3),              # Offset the text slightly to the right\n",
    "            #    textcoords='offset points',  # Use offset from the point to place the text\n",
    "            #    ha='left',                   # Horizontal alignment of the text\n",
    "            #)\n",
    "\n",
    "        plt.xlabel('Epoch@Batch')  # Label for the x-axis\n",
    "        plt.ylabel('Accuracy [%]')     # Label for the y-axis\n",
    "        extra_title = \"\" if elapsed_time == 0 else f\" (took {elapsed_time:.2f}s)\"\n",
    "        plt.title('Accuracy per Epoch and Batch' + extra_title)  # Title of the plot\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "        plt.grid(True)  # Show grid\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1)) # legend on the right\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust plot area size to leave space for the legend\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "''' Calculate the image size after a layer has been applied\n",
    "    assume all operations to be x/y symmetric '''\n",
    "def output_size(h_in, kernel_size, stride, padding):\n",
    "    h_out = (h_in + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    #w_out = (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    return h_out # = w_out\n",
    "    \n",
    "def create_random_population(pop_size: int, \n",
    "                             max_2d_block_count: int = 3, \n",
    "                             max_kernel_size: int = 11,\n",
    "                             name_prefix=\"nn\",\n",
    "                             device=\"cpu\",\n",
    "                             print_summary: bool = True) -> NN_population:\n",
    "    population = []\n",
    "    for i in range(pop_size):\n",
    "        genes_2d_block = []\n",
    "        input_image_size = IMAGE_HEIGHT # = IMAGE_WIDTH (assumed)\n",
    "        name=name_prefix+str(i)\n",
    "        if print_summary: print(f\"Individual '{name}' <-- ({input_image_size} x {input_image_size})\")\n",
    "        for j in range(random.randint(1, max_2d_block_count)):\n",
    "            if print_summary: print(f\"\\tBlock {j}\")\n",
    "            conv_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)))\n",
    "            conv_stride=random.randint(1,conv_kernel_size)\n",
    "            conv_padding=random.randint(0,conv_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "            # update input image size after convolutional layer\n",
    "            input_image_size = output_size(input_image_size, conv_kernel_size, conv_stride, conv_padding)\n",
    "            pool_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)))\n",
    "            pool_stride=max(random.randint(1,pool_kernel_size), random.randint(1,pool_kernel_size))\n",
    "            pool_padding=random.randint(0,pool_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "            block = Gene_2d_block(\n",
    "                out_channels=random.randint(3,15), # not fine-tuned\n",
    "                conv_kernel_size=conv_kernel_size,\n",
    "                conv_padding=conv_padding,\n",
    "                conv_stride=conv_stride,\n",
    "                pool_kernel_size=pool_kernel_size,\n",
    "                pool_padding=pool_padding,\n",
    "                pool_stride=pool_stride\n",
    "            )\n",
    "            genes_2d_block.append(block)\n",
    "            input_image_size = output_size(input_image_size, pool_kernel_size, pool_stride, pool_padding)\n",
    "            if print_summary: print(f\"{block.toString(tab_count=2)} --> ({input_image_size} x {input_image_size})\")\n",
    "        population.append(NN_individual(genes_2d_block=genes_2d_block, name=name, device=device))\n",
    "    return NN_population(population)\n",
    "testPop = create_random_population(pop_size=7, max_2d_block_count=3, print_summary=False)\n",
    "try:\n",
    "    for ind in testPop:\n",
    "        ind.eval()\n",
    "        with torch.inference_mode():\n",
    "            ind(testX)\n",
    "except:\n",
    "    print(\"oh, oh! exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Evaluation\n",
    "We want a population that:\n",
    "- achieves high (validation/test data) accuracy after training\n",
    "    - the final accuracy `acc(NN1(t_final))` of an individual `NN1` is used\n",
    "- trains fast, i.e. takes little CPU time to achieve high accuracy called **Running Accuracy**\n",
    "    - the individual's accuracy `acc(NN1(t))` is summed over given timestamps `t`, like `Σ_t{acc(NN1(t))}`\n",
    "    - possibly we want to value early accuracy more, summing `Σ_t{acc(NN1(t))/t}` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import functional\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_one_batch(ind: NN_individual, # <- model to be trained in-place\n",
    "                          X, y,               # <- train batch, e.g. X.shape = [32, 1, 28, 28]\n",
    "                          ) -> tuple[float, float]:\n",
    "  train_loss = 0\n",
    "  start_time = time.perf_counter() # Start timing\n",
    "  ind.train()\n",
    "  X, y = X.to(ind.device), y.to(ind.device)\n",
    "  y_pred = ind(X)\n",
    "  loss = ind.loss_fn(y_pred, y)\n",
    "  train_loss += loss\n",
    "  ind.optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  ind.optimizer.step()\n",
    "  end_time = time.perf_counter() # Stop timing\n",
    "  return (train_loss, end_time-start_time)\n",
    "\n",
    "def test_model(ind: NN_individual,            # <- model to be tested\n",
    "               test_dl,                       # <- test dataloader (= multiple batches)\n",
    "               ) -> tuple[float, float]:      # -> return (loss_total, acc_total)\n",
    "  loss_total, acc_total = 0, 0\n",
    "  ind.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X, y) in enumerate(test_dl):\n",
    "      X, y = X.to(ind.device), y.to(ind.device)\n",
    "      preds = ind(X)\n",
    "      loss_batch = ind.loss_fn(preds, y)\n",
    "      loss_total += loss_batch\n",
    "      acc_batch = functional.accuracy(preds, y, task=\"multiclass\", num_classes=CLASSIFICATION_CATEGORIES_COUNT)\n",
    "      acc_total += acc_batch\n",
    "\n",
    "    loss_total /= len(test_dl)\n",
    "    acc_total /= len(test_dl)\n",
    "  return (loss_total, acc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the whole population simultaneously, populating the individuals' fitness value parameters:\n",
    "- `train_losses` ... a dictionary filled with the train loss function results for each batch (independent of how often we test, because it comes for free)\n",
    "- `test_losses` ... same as above but evaluating on test data instead, and only whenever we choose to test (obviously; this is not free)\n",
    "- `accs` ... a dictionary with same keys as `test_losses`, filled with the fraction of correct model predictions by total number of predictions\n",
    "- `acc` ... a single number - the most recent accuracy (defined similarly as `accs`)\n",
    "- `running_acc` ... a single number - the sum of all known accuracies (i.e. at all times where we tested), divided by the time\n",
    "    - here, we exclude the first accuracy because the division is very big in that case, and accuracy only depends mainly on weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def train_and_evaluate_gen(pop: NN_population,\n",
    "                           train_dl,  # <- train dataloader\n",
    "                           test_dl,   # <- test dataloader\n",
    "                           testing_interval = 300,      # <- after how many batches should we test an individual\n",
    "                           testing_data_fraction = 1.0, # <- amount of test_dl to be used (1=100% takes a lot of time)\n",
    "                           epochs = 5,\n",
    "                           live_plot = True,\n",
    "                           no_plot = False):\n",
    "  start_time = time.perf_counter() # Start timing\n",
    "\n",
    "  # prepare the reduced testing data loader\n",
    "  total_batches = len(test_dl)\n",
    "  num_batches_to_sample = int(total_batches * testing_data_fraction) # number of batches to select\n",
    "  random_batch_indices = random.sample(range(total_batches), num_batches_to_sample) # random indices (without replacement)\n",
    "  test_subset = Subset(test_dl.dataset, random_batch_indices)\n",
    "  test_subset_dl = DataLoader(test_subset, batch_size=test_dl.batch_size, shuffle=False, num_workers=test_dl.num_workers)\n",
    "\n",
    "  # re-initialize pop's fitness values:\n",
    "  for ind in pop:\n",
    "    ind.acc, ind.running_acc = 0, 0\n",
    "    ind.train_losses, ind.test_losses, ind.accs = {}, {}, {}\n",
    "  \n",
    "  # train each individual \"simultaneously\" by making the epoch-loop the outer one\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"*** Commencing epoch {epoch+1} / {epochs} for {len(pop)} individuals, one line each. ***\")\n",
    "    for i in range(len(pop)):\n",
    "      #print(f\"Training {pop[i].name} for {len(train_dl)} batches (of size {train_dl.batch_size}): \", end='')\n",
    "      for batch, (X, y) in tqdm(enumerate(train_dl)):\n",
    "        # train the model (update the weights and biases of the NN pop[i])\n",
    "        pop[i].train_losses[f\"e_{epoch}@b_{batch}\"], elapsed_batch_training_time = train_model_one_batch(pop[i], X=X, y=y)\n",
    "        pop[i].elapsed_training_time += elapsed_batch_training_time\n",
    "        if batch % testing_interval == 0: \n",
    "          # test the model and store the results\n",
    "          pop[i].test_losses[f\"e_{epoch}@b_{batch}\"], pop[i].accs[f\"e_{epoch}@b_{batch}\"] = test_model(pop[i], test_dl=test_subset_dl)\n",
    "          if batch != 0: # don't use the start/benchmark test as this depends mostly on luck of weight initialization\n",
    "            pop[i].running_acc += pop[i].accs[f\"e_{epoch}@b_{batch}\"] / pop[i].elapsed_training_time\n",
    "          if live_plot and not no_plot:\n",
    "            clear_output(wait=True)\n",
    "            pop.plot_accs(time.perf_counter() - start_time)\n",
    "      pop[i].test_losses[f\"e_{epoch}@end\"], pop[i].accs[f\"e_{epoch}@end\"] = test_model(pop[i], test_dl=test_dl) # latest precise values\n",
    "      pop[i].acc = pop[i].accs[f\"e_{epoch}@end\"] # store the very last known accuracy\n",
    "      if not live_plot and not no_plot:\n",
    "        clear_output(wait=True)\n",
    "        pop.plot_accs(time.perf_counter() - start_time)\n",
    "    # here we could select directly, i.e. before the whole train_dl over max_epochs no. of iterations has been trained\n",
    "\n",
    "  if not no_plot:\n",
    "    clear_output(wait=True)\n",
    "    pop.plot_accs(time.perf_counter() - start_time)\n",
    "  else:\n",
    "    print(f\"This took {time.perf_counter() - start_time:.2f}s.\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Example Data\n",
    "Use the example dataloaders to populate the fitness values for a generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = create_random_population(pop_size=6, max_2d_block_count=4, max_kernel_size=7, name_prefix=\"gen0.\", device=device, print_summary=False)\n",
    "pop[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb40b33575864d178d20b3404ef14536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('a', 'b', 'c'), value='a'), Dropdown(description='y',…"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "%matplotlib inline\n",
    "\n",
    "columns=['a','b','c']\n",
    "data = np.cumsum(np.random.rand(10,3),axis=1)\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "\n",
    "def g(x,y):\n",
    "    plt.scatter(df[x], df[y])\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(g, x=columns, y=columns)\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Commencing epoch 1 / 1 for 6 individuals, one line each. ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8b60ec27574dac864f81117153f867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef43c4c455d14b628abb7d2cbec957be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734816b7f03b4f658c606690a81a215c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2916cd772364fb09f0b9f4dd465ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbe53b61bc1498aaa5f4c8ba15b6c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099d585bb05044bbacf7bf656391f9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate_gen(pop, train_dl_f_mnist, test_dl_f_mnist, testing_interval=100, epochs=1, testing_data_fraction=.1, live_plot=False, no_plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
