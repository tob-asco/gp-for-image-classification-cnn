{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121 running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"{torch.__version__} running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Run for Image Classification\n",
    "Your problem needs to fulfill the following criteria.\n",
    "1. It is an image classification problem.\n",
    "2. You supply marked training images and marked validation images.\n",
    "\n",
    "Within those, the run is flexible and adapts itself to your problem.\n",
    "Now, please describe your images and problem by setting those global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28 # <-- number of width-pixels\n",
    "IMAGE_HEIGHT = 28 # <-- number of height-pixels\n",
    "COLOUR_CHANNEL_COUNT = 1 # <-- RGB images would have 3\n",
    "CLASSIFICATION_CATEGORIES_COUNT = 10 # <-- the amount of possible categories of which each image shall be marked with one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Example Data\n",
    "To check the code, we prepare example data: Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data (and not testing data)\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
    "print(f\"train_data.classes = {train_data.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dl_f_mnist.batch_size = 32\n",
      "len(next(iter(train_dl_f_mnist))) = 2\n",
      "next(iter(train_dl_f_mnist))[0].shape = torch.Size([32, 1, 28, 28])\n",
      "len(train_dl_f_mnist) = 1875, len(test_dl_f_mnist) = 313\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "MINI_BATCH_SIZE = 32 # constant for now\n",
    "\n",
    "# Turn datasets into iterables (batches), shuffeling train data every epoch (test data not)\n",
    "train_dl_f_mnist = DataLoader(train_data, batch_size=MINI_BATCH_SIZE, shuffle=True)\n",
    "test_dl_f_mnist = DataLoader(test_data, batch_size=MINI_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"train_dl_f_mnist.batch_size = {train_dl_f_mnist.batch_size}\") \n",
    "print(f\"len(next(iter(train_dl_f_mnist))) = {len(next(iter(train_dl_f_mnist)))}\") \n",
    "print(f\"next(iter(train_dl_f_mnist))[0].shape = {next(iter(train_dl_f_mnist))[0].shape}\") \n",
    "print(f\"len(train_dl_f_mnist) = {len(train_dl_f_mnist)}, len(test_dl_f_mnist) = {len(test_dl_f_mnist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = 2 # index in the batch, 0 .. 31\n",
    "train_features_batch, train_labels_batch = next(iter(train_dl_f_mnist))\n",
    "print(f\"Image shape: {train_features_batch[image_index].shape}\")\n",
    "plt.imshow(train_features_batch[image_index].squeeze(), cmap=\"gray\") # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(str(train_labels_batch[image_index].item())+\" i.e. \"+train_data.classes[train_labels_batch[image_index].item()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Individuals' Class\n",
    "- The Hyperparamters should be passed to the constructor in a way that is both convenient for GP and for PyTorch.\n",
    "- I think I want to define a class for one `nn.Sequential` 2d-block\n",
    "    - All possible instances should be concatenable with all possible instances\n",
    "- Then, an individual is built from the concatenation of many such blocks, plus data preparation and final f.c. layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "define a λ nn.Module that creates an nn layer from a given function\n",
    "this is handy for nn.Sequential usage '''\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "'''\n",
    "create a function to reshape the (28x28) image input data '''\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testX.shape = torch.Size([1, 28, 28])\n",
      "testX[:,:3]: tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431,\n",
      "          -1.6047, -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594,\n",
      "          -0.7688,  0.7624,  1.6423, -0.1596, -0.4974,  0.4396, -0.7581,\n",
      "           1.0783,  0.8008,  1.6806,  1.2791,  1.2964,  0.6105,  1.3347],\n",
      "         [-0.2316,  0.0418, -0.2516,  0.8599, -1.3847, -0.8712, -0.2234,\n",
      "           1.7174,  0.3189, -0.4245,  0.3057, -0.7746, -1.5576,  0.9956,\n",
      "          -0.8798, -0.6011, -1.2742,  2.1228, -1.2347, -0.4879, -0.9138,\n",
      "          -0.6581,  0.0780,  0.5258, -0.4880,  1.1914, -0.8140, -0.7360],\n",
      "         [-1.4032,  0.0360, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,\n",
      "           1.3835,  1.4451,  0.8564,  2.2181,  0.5232,  0.3466, -0.1973,\n",
      "          -1.0546,  1.2780, -0.1722,  0.5238,  0.0566,  0.4263,  0.5750,\n",
      "          -0.6417, -2.2064, -0.7508,  0.0109, -0.3387, -1.3407, -0.5854]]])\n",
      "testBlock1(testX).shape = torch.Size([5, 14, 14])\n",
      "testBlock1(testX)[:1,:3]: tensor([[[0.5044, 0.5792, 0.5977, 1.2482, 0.1264, 0.3826, 0.4878, 0.7264,\n",
      "          0.4335, 0.1201, 0.4713, 0.6797, 0.5033, 0.1936],\n",
      "         [0.6928, 0.2455, 1.2562, 0.3254, 1.0197, 0.6961, 0.4173, 0.7566,\n",
      "          0.2675, 0.6663, 1.1496, 0.5233, 0.4469, 0.6933],\n",
      "         [0.5437, 0.3650, 1.1003, 0.3155, 0.4760, 1.1148, 0.7317, 0.7851,\n",
      "          0.0000, 1.1040, 0.4643, 0.2110, 0.0000, 0.5719]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Sequential_block_2d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_channels: int, # the number of output neurons after the full block\n",
    "                 in_channels: int = 1, # should not be set here, but is set in Individual's __init__()\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=conv_kernel_size,\n",
    "                      stride=conv_stride,\n",
    "                      padding=conv_padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size,\n",
    "                         stride=pool_stride,\n",
    "                         padding=pool_padding)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# generate some testing blocks (TODO: write actual unit tests)\n",
    "testBlock1 = Sequential_block_2d(in_channels=1,out_channels=5)\n",
    "testBlock2 = Sequential_block_2d(in_channels=5,out_channels=3)\n",
    "torch.manual_seed(42)\n",
    "testX = torch.randn(COLOUR_CHANNEL_COUNT,IMAGE_WIDTH,IMAGE_HEIGHT)\n",
    "testBlock1, testBlock2, testX, testBlock1(testX)\n",
    "print(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\n",
    "print(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_max_pool_adaptive output shape is torch.Size([3, 5, 5])\n",
      "flatten output shape is torch.Size([75])\n",
      "lin output shape is torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0477, -0.0685,  0.0542, -0.0027, -0.1166, -0.0345, -0.0618,  0.1197,\n",
       "         0.0490,  0.1229], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, blocks_2d: list[Sequential_block_2d],\n",
    "                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\n",
    "                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\n",
    "                 fin_res: int = 5): # output dimension of the final max pooling \n",
    "                                    # producing size (fin_res * fin_res)\n",
    "        super().__init__()\n",
    "        self.blocks_2d = blocks_2d\n",
    "        # add a final max pool\n",
    "        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\n",
    "        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\n",
    "                      out_features = out_dimensions)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.last_max_pool_adaptive(x)\n",
    "        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lin(x)\n",
    "        print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\n",
    "testIndividual(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_channels of the first needs to match in_channels of the second!\n",
      "in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\n"
     ]
    }
   ],
   "source": [
    "# this way, adjacent genes (= 2d_blocks) need to have the correct dimensions\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=1,out_channels=2), Sequential_block_2d(in_channels=3, out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"out_channels of the first needs to match in_channels of the second!\")\n",
    "\n",
    "# but there's more redundancy:\\the first gene needs to have the same number of in_channels as there are colour channels\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=COLOUR_CHANNEL_COUNT + 1,out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brainstorm on How to Encode Individuals\n",
    "We need to talk about this right now because we want to adapt our `NN_individual.blocks_2d` definition according to it.\n",
    "The options are:\n",
    "1. We specify `Sequential_block_2d.in_channels` and `~.out_channels` separately for each individual and only allow concatenation if the criteria are met. This is probably not super clever...\n",
    "2. Genotype-closure: The parameters that are adapted through GP will never leave the space of syntacticly correct indivuals\n",
    "    - The first gene is not allowed to choose `~.in_channels`, it must match `COLOUR_CHANNEL_COUNT`\n",
    "    - Every gene but the first is not allowed to choose `~.in_channels`, it must match `~.out_channels` of the prior gene\n",
    "    - How will this change the gene class `Sequential_block_2d`?\n",
    "        - Set `Sequential_block_2d.in_channels` only programatically, in `NN_individual.__init__()`\n",
    "        - Don't even let the genes inherit from `nn.Module`, only the individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the genetic information of one 2d block\n",
    "class Gene_2d_block:\n",
    "    def __init__(self,\n",
    "                 out_channels: int,\n",
    "                 in_channels: int = None, # set in the individual's constructor\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_stride = conv_stride\n",
    "        self.conv_padding = conv_padding\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.pool_padding = pool_padding\n",
    "\n",
    "    def toString(self, tab_count: int = 0):\n",
    "        indentation = \"\"\n",
    "        for tab in range(tab_count): indentation += f\"\\t\"\n",
    "        return f\"{indentation}out_channels = {self.out_channels}\\n\"+\\\n",
    "        f\"{indentation}conv_2d (kernel, stride, padding) =\\t({self.conv_kernel_size}, {self.conv_stride}, {self.conv_padding})\\n\"+\\\n",
    "        f\"{indentation}max_pool_2d (kernel, stride, padding) =\\t({self.pool_kernel_size}, {self.pool_stride}, {self.pool_padding})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NN_individual(\n",
       "   (blocks_2d): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU()\n",
       "       (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU()\n",
       "       (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "   )\n",
       "   (flatten): Flatten(start_dim=0, end_dim=-1)\n",
       "   (lazyLin): Linear(in_features=343, out_features=10, bias=True)\n",
       " ),\n",
       " tensor([-0.0134, -0.0387, -0.0594,  0.0336,  0.0421,  0.0608, -0.1380, -0.0417,\n",
       "          0.1144,  0.0008], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \n",
    "        super().__init__()\n",
    "        self.name=name # a name for easier tracking inside a GP run\n",
    "        ''' build the full sequential from the gene information (genes_2d_block) '''\n",
    "        self.blocks_2d = nn.Sequential()\n",
    "        # the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "        # the others need to have as in_channels the number of out_channels from the previous block\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            self.blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lazyLin(x)\n",
    "        #print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Code\n",
    "Now we need to decide the genetic stuff.\n",
    "1. Initial Population\n",
    "2. Fitness Measure\n",
    "3. Selection\n",
    "4. Genetic Operators\n",
    "    - Cloning or Crossover\n",
    "    - Mutation\n",
    "    \n",
    "### Hyperparameter-landscape is vast. Here's a list:\n",
    "- Net architecture\n",
    "    - kind of layers, number of layers\n",
    "        - for convolution/pooling: kernel size, stride, padding, (dilation) **[implemented]**\n",
    "    - number of neurons per layer\n",
    "    - activation function for each layer\n",
    "- cost function\n",
    "    - base term (e.g. square cost, log-likelihood, cross-entropy, ect.)\n",
    "    - toppings \n",
    "        - regularization of weights (L2, L1, dropout, etc.)\n",
    "- weights and biases optimization technique (= optimizer)\n",
    "    - SGD (= stochastic gradient descent)\n",
    "    - Hessian technique, i.e. momentum-based descent\n",
    "    - PyTorch's various other (e.g. *Adam* optimizer)\n",
    "- learning parameters\n",
    "    - η ... learning rate\n",
    "        - constant, or epoch-dependent, or accuracy-dependent, or a mix\n",
    "    - \\# of epochs\n",
    "        - constant, or early stopping\n",
    "    - (`mini_batch_size` - this one might be canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Within-One-Gen-Constant Hyperparameters in `NN_individual`\n",
    "We now bake:\n",
    "- the individual-specific, hyperparameters (that don't change within one gen)\n",
    "- the fitness dictionaries\n",
    "\n",
    "into parameters of the `NN_individual` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NN_individual(\n",
       "   (blocks_2d): Sequential()\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (lazyLin): Linear(in_features=784, out_features=10, bias=True)\n",
       "   (loss_fn): CrossEntropyLoss()\n",
       " ),\n",
       " tensor([[ 0.4219,  0.0387,  0.6340,  0.2156,  0.1363,  0.3297,  0.0587,  0.6220,\n",
       "           0.4417, -0.9403]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, \n",
    "                 genes_2d_block: list[Gene_2d_block] = [],  # <- genes for the 2d convolution blocks\n",
    "                 name: str = \"nn0\",                         # <- an inside-population-unique name\n",
    "                 optimizer_gene: str = \"sgd\",               # <- gene for the optimizer\n",
    "                 lr = .1,                                   # <- learning rate\n",
    "                 loss_fn = nn.CrossEntropyLoss(),           # <- loss function\n",
    "                 device = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.blocks_2d = self.__genotype_phenotype_mapping_2d_blocks__(genes_2d_block)\n",
    "        #self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default is: start_dim = 1\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1) # default is: start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "        self.name = name\n",
    "        self.optimizer = self.__genotype_phenotype_mapping_optimizer__(optimizer_gene, lr)\n",
    "        self.lr = lr\n",
    "        self.loss_fn = loss_fn\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.acc = 0\n",
    "        self.running_acc = 0\n",
    "        self.train_losses = {}\n",
    "        self.test_losses = {}\n",
    "        self.accs = {}\n",
    "        \n",
    "    def __genotype_phenotype_mapping_2d_blocks__(self, genes_2d_block: list[Gene_2d_block]):\n",
    "        ''' build 'n return the full sequential from the gene information (genes_2d_block) \n",
    "            the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "            the others need to have as in_channels the number of out_channels from the previous block\n",
    "            there's a nn.Module (Lazy*) that automatically infers the number of in_channels - not used here '''\n",
    "        blocks_2d = nn.Sequential()\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        return blocks_2d\n",
    "    \n",
    "    def __genotype_phenotype_mapping_optimizer__(self, optimizer_gene: str, lr: float):\n",
    "        if optimizer_gene.lower() == \"sgd\":\n",
    "            return torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        return torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lazyLin(x)\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual()#genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate the image size after a layer has been applied\n",
    "    assume all operations to be x/y symmetric '''\n",
    "def output_size(h_in, kernel_size, stride, padding):\n",
    "    h_out = (h_in + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    #w_out = (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    \n",
    "    return h_out # = w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_random_population(pop_size: int, \n",
    "                             max_2d_block_count: int = 3, \n",
    "                             max_kernel_size: int = 11,\n",
    "                             name_prefix=\"nn\",\n",
    "                             device=\"cpu\",\n",
    "                             print_summary: bool = True) -> list[NN_individual]:\n",
    "    population = []\n",
    "    for i in range(pop_size):\n",
    "        genes_2d_block = []\n",
    "        input_image_size = IMAGE_HEIGHT # = IMAGE_WIDTH (assumed)\n",
    "        name=name_prefix+str(i)\n",
    "        if print_summary: print(f\"Individual '{name}' <-- ({input_image_size} x {input_image_size})\")\n",
    "        for j in range(random.randint(1, max_2d_block_count)):\n",
    "            if print_summary: print(f\"\\tBlock {j}\")\n",
    "            conv_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)))\n",
    "            conv_stride=random.randint(1,conv_kernel_size)\n",
    "            conv_padding=random.randint(0,conv_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "            # update input image size after convolutional layer\n",
    "            input_image_size = output_size(input_image_size, conv_kernel_size, conv_stride, conv_padding)\n",
    "            pool_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)))\n",
    "            pool_stride=max(random.randint(1,pool_kernel_size), random.randint(1,pool_kernel_size))\n",
    "            pool_padding=random.randint(0,pool_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "            block = Gene_2d_block(\n",
    "                out_channels=random.randint(3,15), # not fine-tuned\n",
    "                conv_kernel_size=conv_kernel_size,\n",
    "                conv_padding=conv_padding,\n",
    "                conv_stride=conv_stride,\n",
    "                pool_kernel_size=pool_kernel_size,\n",
    "                pool_padding=pool_padding,\n",
    "                pool_stride=pool_stride\n",
    "            )\n",
    "            genes_2d_block.append(block)\n",
    "            input_image_size = output_size(input_image_size, pool_kernel_size, pool_stride, pool_padding)\n",
    "            if print_summary: print(f\"{block.toString(tab_count=2)} --> ({input_image_size} x {input_image_size})\")\n",
    "        population.append(NN_individual(genes_2d_block=genes_2d_block, name=name, device=device))\n",
    "    return population\n",
    "testPop = create_random_population(pop_size=7, max_2d_block_count=3, print_summary=False)\n",
    "try:\n",
    "    for ind in testPop:\n",
    "        ind.eval()\n",
    "        with torch.inference_mode():\n",
    "            ind(testX)\n",
    "except:\n",
    "    print(\"oh, oh! exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Evaluation\n",
    "We want a population that:\n",
    "- achieves high (validation/test data) accuracy after training\n",
    "    - the final accuracy `acc(NN1(t_final))` of an individual `NN1` is used\n",
    "- trains fast, i.e. takes little CPU time to achieve high accuracy called **Running Accuracy**\n",
    "    - the individual's accuracy `acc(NN1(t))` is summed over given timestamps `t`, like `Σ_t{acc(NN1(t))}`\n",
    "    - possibly we want to value early accuracy more, summing `Σ_t{acc(NN1(t))/t}` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_one_batch(ind: NN_individual, # <- model to be trained in-place\n",
    "                          X, y,               # <- train batch, e.g. X.shape = [32, 1, 28, 28]\n",
    "                          ) -> float:\n",
    "  train_loss = 0\n",
    "  ind.train()\n",
    "  X, y = X.to(ind.device), y.to(ind.device)\n",
    "  y_pred = ind(X)\n",
    "  loss = ind.loss_fn(y_pred, y)\n",
    "  train_loss += loss\n",
    "  ind.optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  ind.optimizer.step()\n",
    "  return train_loss # <- no need to return ind, it is modified in-place\n",
    "\n",
    "def test_model(ind: NN_individual,            # <- model to be tested\n",
    "               test_dl,                       # <- test dataloader (= multiple batches)\n",
    "               ) -> tuple[float, float]:      # -> return (loss_total, acc_total)\n",
    "  loss_total, acc_total = 0, 0\n",
    "  ind.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X, y) in enumerate(test_dl):\n",
    "      X, y = X.to(ind.device), y.to(ind.device)\n",
    "      preds = ind(X)\n",
    "      loss_batch = ind.loss_fn(preds, y)\n",
    "      loss_total += loss_batch\n",
    "      acc_batch = functional.accuracy(preds, y, task=\"multiclass\", num_classes=CLASSIFICATION_CATEGORIES_COUNT)\n",
    "      acc_total += acc_batch\n",
    "\n",
    "    loss_total /= len(test_dl)\n",
    "    acc_total /= len(test_dl)\n",
    "  return (loss_total, acc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the whole population simultaneously, populating the individuals' fitness value parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_gen(pop: list[NN_individual],\n",
    "                           train_dl,  # <- train dataloader\n",
    "                           test_dl,   # <- test dataloader\n",
    "                           max_epochs = 5):\n",
    "  # re-initialize pop's fitness values:\n",
    "  for ind in pop:\n",
    "    ind.acc, ind.running_acc = 0, 0\n",
    "    ind.train_losses, ind.test_losses, ind.accs = {}, {}, {}\n",
    "    #for epoch in range(max_epochs):\n",
    "    #  for batch, (X, y) in enumerate(train_dl):\n",
    "    #    ind.train_losses[f\"e_{epoch}@b_{batch}\"] = 0\n",
    "    #    ind.test_losses[f\"e_{epoch}@b_{batch}\"] = 0\n",
    "    #    ind.accs[f\"e_{epoch}@b_{batch}\"] = 0\n",
    "  \n",
    "  # train each individual \"simultaneously\" by making the epoch-loop the outer one\n",
    "  for epoch in range(max_epochs):\n",
    "    print(f\"*** Commencing epoch {epoch} / {max_epochs}. ***\")\n",
    "    for batch, (X, y) in enumerate(train_dl):\n",
    "      for i in range(len(pop)):\n",
    "        # update the weights and biases of the NN pop[i]\n",
    "        train_loss = train_model_one_batch(pop[i], X=X, y=y)\n",
    "        # test the model (every once and while?)\n",
    "        test_loss, acc = test_model(pop[i], test_dl=test_dl)\n",
    "        # Print out what's happening\n",
    "        print(f\"e_{epoch}@b_{batch} | {pop[i].name}: test loss {test_loss:.3f}, accuracy {acc*100:.2f}%\")\n",
    "        # store the values for fitness evaluation (NN_individual.name is assumed a unique identifier here)\n",
    "        pop[i].train_losses[f\"e_{epoch}@b_{batch}\"] = train_loss\n",
    "        pop[i].test_losses[f\"e_{epoch}@b_{batch}\"] = test_loss\n",
    "        pop[i].accs[f\"e_{epoch}@b_{batch}\"] = acc\n",
    "      # here we could select directly, i.e. before the whole train_dl over max_epochs no. of iterations has been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Example Data\n",
    "Use the example dataloaders to populate the fitness values for a generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = create_random_population(pop_size=3, max_2d_block_count=3, max_kernel_size=7, name_prefix=\"gen0.\", device=device, print_summary=False)\n",
    "pop[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Commencing epoch 0 / 2. ***\n",
      "e_0@b_0 | gen0.0: test loss 2.278, accuracy 19.18%\n",
      "e_0@b_0 | gen0.1: test loss 2.295, accuracy 9.99%\n",
      "e_0@b_0 | gen0.2: test loss 2.317, accuracy 9.99%\n",
      "e_0@b_1 | gen0.0: test loss 2.263, accuracy 10.00%\n",
      "e_0@b_1 | gen0.1: test loss 2.290, accuracy 9.99%\n",
      "e_0@b_1 | gen0.2: test loss 2.309, accuracy 9.99%\n",
      "e_0@b_2 | gen0.0: test loss 2.253, accuracy 12.25%\n",
      "e_0@b_2 | gen0.1: test loss 2.287, accuracy 10.10%\n",
      "e_0@b_2 | gen0.2: test loss 2.308, accuracy 9.99%\n",
      "e_0@b_3 | gen0.0: test loss 2.243, accuracy 11.80%\n",
      "e_0@b_3 | gen0.1: test loss 2.284, accuracy 10.53%\n",
      "e_0@b_3 | gen0.2: test loss 2.305, accuracy 9.99%\n",
      "e_0@b_4 | gen0.0: test loss 2.232, accuracy 12.69%\n",
      "e_0@b_4 | gen0.1: test loss 2.279, accuracy 10.06%\n",
      "e_0@b_4 | gen0.2: test loss 2.303, accuracy 9.99%\n",
      "e_0@b_5 | gen0.0: test loss 2.223, accuracy 23.58%\n",
      "e_0@b_5 | gen0.1: test loss 2.276, accuracy 10.75%\n",
      "e_0@b_5 | gen0.2: test loss 2.302, accuracy 9.99%\n",
      "e_0@b_6 | gen0.0: test loss 2.209, accuracy 23.34%\n",
      "e_0@b_6 | gen0.1: test loss 2.272, accuracy 12.05%\n",
      "e_0@b_6 | gen0.2: test loss 2.301, accuracy 9.99%\n",
      "e_0@b_7 | gen0.0: test loss 2.204, accuracy 18.65%\n",
      "e_0@b_7 | gen0.1: test loss 2.269, accuracy 9.98%\n",
      "e_0@b_7 | gen0.2: test loss 2.302, accuracy 9.99%\n",
      "e_0@b_8 | gen0.0: test loss 2.177, accuracy 18.07%\n",
      "e_0@b_8 | gen0.1: test loss 2.262, accuracy 10.33%\n",
      "e_0@b_8 | gen0.2: test loss 2.301, accuracy 9.99%\n",
      "e_0@b_9 | gen0.0: test loss 2.155, accuracy 26.57%\n",
      "e_0@b_9 | gen0.1: test loss 2.256, accuracy 11.51%\n",
      "e_0@b_9 | gen0.2: test loss 2.300, accuracy 10.53%\n",
      "e_0@b_10 | gen0.0: test loss 2.130, accuracy 29.95%\n",
      "e_0@b_10 | gen0.1: test loss 2.252, accuracy 14.52%\n",
      "e_0@b_10 | gen0.2: test loss 2.300, accuracy 10.02%\n",
      "e_0@b_11 | gen0.0: test loss 2.093, accuracy 29.58%\n",
      "e_0@b_11 | gen0.1: test loss 2.241, accuracy 18.07%\n",
      "e_0@b_11 | gen0.2: test loss 2.298, accuracy 12.49%\n",
      "e_0@b_12 | gen0.0: test loss 2.089, accuracy 21.19%\n",
      "e_0@b_12 | gen0.1: test loss 2.242, accuracy 12.16%\n",
      "e_0@b_12 | gen0.2: test loss 2.300, accuracy 12.92%\n",
      "e_0@b_13 | gen0.0: test loss 2.037, accuracy 35.00%\n",
      "e_0@b_13 | gen0.1: test loss 2.226, accuracy 21.75%\n",
      "e_0@b_13 | gen0.2: test loss 2.298, accuracy 9.99%\n",
      "e_0@b_14 | gen0.0: test loss 1.978, accuracy 42.29%\n",
      "e_0@b_14 | gen0.1: test loss 2.213, accuracy 20.57%\n",
      "e_0@b_14 | gen0.2: test loss 2.298, accuracy 11.75%\n",
      "e_0@b_15 | gen0.0: test loss 1.982, accuracy 27.68%\n",
      "e_0@b_15 | gen0.1: test loss 2.206, accuracy 15.01%\n",
      "e_0@b_15 | gen0.2: test loss 2.299, accuracy 9.99%\n",
      "e_0@b_16 | gen0.0: test loss 1.932, accuracy 28.63%\n",
      "e_0@b_16 | gen0.1: test loss 2.194, accuracy 21.78%\n",
      "e_0@b_16 | gen0.2: test loss 2.299, accuracy 9.99%\n",
      "e_0@b_17 | gen0.0: test loss 1.838, accuracy 38.57%\n",
      "e_0@b_17 | gen0.1: test loss 2.180, accuracy 22.82%\n",
      "e_0@b_17 | gen0.2: test loss 2.298, accuracy 9.99%\n",
      "e_0@b_18 | gen0.0: test loss 1.754, accuracy 37.57%\n",
      "e_0@b_18 | gen0.1: test loss 2.157, accuracy 23.03%\n",
      "e_0@b_18 | gen0.2: test loss 2.296, accuracy 9.99%\n",
      "e_0@b_19 | gen0.0: test loss 1.700, accuracy 44.25%\n",
      "e_0@b_19 | gen0.1: test loss 2.143, accuracy 27.66%\n",
      "e_0@b_19 | gen0.2: test loss 2.294, accuracy 9.99%\n",
      "e_0@b_20 | gen0.0: test loss 1.758, accuracy 38.36%\n",
      "e_0@b_20 | gen0.1: test loss 2.112, accuracy 20.89%\n",
      "e_0@b_20 | gen0.2: test loss 2.291, accuracy 15.74%\n",
      "e_0@b_21 | gen0.0: test loss 1.727, accuracy 31.09%\n",
      "e_0@b_21 | gen0.1: test loss 2.097, accuracy 34.75%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_evaluate_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl_f_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl_f_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[169], line 23\u001b[0m, in \u001b[0;36mtrain_and_evaluate_gen\u001b[0;34m(pop, train_dl, test_dl, max_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_model_one_batch(pop[i], X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# test the model (every once and while?)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m test_loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@b_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpop[i]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: test loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[164], line 22\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(ind, test_dl)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m     21\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dl):\n\u001b[0;32m---> 22\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(ind\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m     preds \u001b[38;5;241m=\u001b[39m ind(X)\n\u001b[1;32m     24\u001b[0m     loss_batch \u001b[38;5;241m=\u001b[39m ind\u001b[38;5;241m.\u001b[39mloss_fn(preds, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_evaluate_gen(pop, train_dl_f_mnist, test_dl_f_mnist, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
