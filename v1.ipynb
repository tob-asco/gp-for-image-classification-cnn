{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121 running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"{torch.__version__} running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Run for Image Classification\n",
    "Your problem needs to fulfill the following criteria.\n",
    "1. It is an image classification problem.\n",
    "2. You supply marked training images and marked validation images.\n",
    "\n",
    "Within those, the run is flexible and adapts itself to your problem.\n",
    "Now, please describe your images and problem by setting those global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28 # <-- number of width-pixels\n",
    "IMAGE_HEIGHT = 28 # <-- number of height-pixels\n",
    "COLOUR_CHANNEL_COUNT = 1 # <-- RGB images would have 3\n",
    "CLASSIFICATION_CATEGORIES_COUNT = 10 # <-- the amount of possible categories of which each image shall be marked with one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Example Data\n",
    "To check the code, we prepare example data: Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data (and not testing data)\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
    "print(f\"train_data.classes = {train_data.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dl_f_mnist.batch_size = 32\n",
      "len(next(iter(train_dl_f_mnist))) = 2\n",
      "next(iter(train_dl_f_mnist))[0].shape = torch.Size([32, 1, 28, 28])\n",
      "len(train_dl_f_mnist) = 1875, len(test_dl_f_mnist) = 313\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "MINI_BATCH_SIZE = 32 # constant for now\n",
    "\n",
    "# Turn datasets into iterables (batches), shuffeling train data every epoch (test data not)\n",
    "train_dl_f_mnist = DataLoader(train_data, batch_size=MINI_BATCH_SIZE, shuffle=True)\n",
    "test_dl_f_mnist = DataLoader(test_data, batch_size=MINI_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"train_dl_f_mnist.batch_size = {train_dl_f_mnist.batch_size}\") \n",
    "print(f\"len(next(iter(train_dl_f_mnist))) = {len(next(iter(train_dl_f_mnist)))}\") \n",
    "print(f\"next(iter(train_dl_f_mnist))[0].shape = {next(iter(train_dl_f_mnist))[0].shape}\") \n",
    "print(f\"len(train_dl_f_mnist) = {len(train_dl_f_mnist)}, len(test_dl_f_mnist) = {len(test_dl_f_mnist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = 2 # index in the batch, 0 .. 31\n",
    "train_features_batch, train_labels_batch = next(iter(train_dl_f_mnist))\n",
    "print(f\"Image shape: {train_features_batch[image_index].shape}\")\n",
    "plt.imshow(train_features_batch[image_index].squeeze(), cmap=\"gray\") # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(str(train_labels_batch[image_index].item())+\" i.e. \"+train_data.classes[train_labels_batch[image_index].item()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Individuals' Class\n",
    "- The Hyperparamters should be passed to the constructor in a way that is both convenient for GP and for PyTorch.\n",
    "- I think I want to define a class for one `nn.Sequential` 2d-block\n",
    "    - All possible instances should be concatenable with all possible instances\n",
    "- Then, an individual is built from the concatenation of many such blocks, plus data preparation and final f.c. layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "define a λ nn.Module that creates an nn layer from a given function\n",
    "this is handy for nn.Sequential usage '''\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "'''\n",
    "create a function to reshape the (28x28) image input data '''\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testX.shape = torch.Size([1, 28, 28])\n",
      "testX[:,:3]: tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431,\n",
      "          -1.6047, -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594,\n",
      "          -0.7688,  0.7624,  1.6423, -0.1596, -0.4974,  0.4396, -0.7581,\n",
      "           1.0783,  0.8008,  1.6806,  1.2791,  1.2964,  0.6105,  1.3347],\n",
      "         [-0.2316,  0.0418, -0.2516,  0.8599, -1.3847, -0.8712, -0.2234,\n",
      "           1.7174,  0.3189, -0.4245,  0.3057, -0.7746, -1.5576,  0.9956,\n",
      "          -0.8798, -0.6011, -1.2742,  2.1228, -1.2347, -0.4879, -0.9138,\n",
      "          -0.6581,  0.0780,  0.5258, -0.4880,  1.1914, -0.8140, -0.7360],\n",
      "         [-1.4032,  0.0360, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,\n",
      "           1.3835,  1.4451,  0.8564,  2.2181,  0.5232,  0.3466, -0.1973,\n",
      "          -1.0546,  1.2780, -0.1722,  0.5238,  0.0566,  0.4263,  0.5750,\n",
      "          -0.6417, -2.2064, -0.7508,  0.0109, -0.3387, -1.3407, -0.5854]]])\n",
      "testBlock1(testX).shape = torch.Size([5, 14, 14])\n",
      "testBlock1(testX)[:1,:3]: tensor([[[1.2048, 0.4604, 0.4993, 0.5266, 0.6208, 0.6759, 0.6755, 0.6569,\n",
      "          0.9566, 0.4436, 1.3937, 0.9365, 0.3784, 0.7696],\n",
      "         [0.3856, 0.0000, 0.2850, 0.7596, 0.8679, 0.2692, 0.3385, 0.2598,\n",
      "          0.4616, 0.3847, 0.0556, 0.7869, 0.0213, 0.4362],\n",
      "         [0.5956, 0.5013, 0.6069, 0.0000, 0.4524, 0.8790, 0.5751, 0.6734,\n",
      "          1.2918, 0.3379, 0.9592, 0.0000, 0.0000, 1.0949]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Sequential_block_2d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_channels: int, # the number of output neurons after the full block\n",
    "                 in_channels: int = 1, # should not be set here, but is set in Individual's __init__()\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=conv_kernel_size,\n",
    "                      stride=conv_stride,\n",
    "                      padding=conv_padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size,\n",
    "                         stride=pool_stride,\n",
    "                         padding=pool_padding)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# generate some testing blocks (TODO: write actual unit tests)\n",
    "testBlock1 = Sequential_block_2d(in_channels=1,out_channels=5)\n",
    "testBlock2 = Sequential_block_2d(in_channels=5,out_channels=3)\n",
    "torch.manual_seed(42)\n",
    "testX = torch.randn(COLOUR_CHANNEL_COUNT,IMAGE_WIDTH,IMAGE_HEIGHT)\n",
    "testBlock1, testBlock2, testX, testBlock1(testX)\n",
    "print(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\n",
    "print(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_max_pool_adaptive output shape is torch.Size([3, 5, 5])\n",
      "flatten output shape is torch.Size([75])\n",
      "lin output shape is torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0382,  0.2572, -0.1972, -0.2364,  0.1726,  0.0854, -0.0477,  0.1086,\n",
       "        -0.2222,  0.0715], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, blocks_2d: list[Sequential_block_2d],\n",
    "                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\n",
    "                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\n",
    "                 fin_res: int = 5): # output dimension of the final max pooling \n",
    "                                    # producing size (fin_res * fin_res)\n",
    "        super().__init__()\n",
    "        self.blocks_2d = blocks_2d\n",
    "        # add a final max pool\n",
    "        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\n",
    "        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\n",
    "                      out_features = out_dimensions)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.last_max_pool_adaptive(x)\n",
    "        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lin(x)\n",
    "        print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\n",
    "testIndividual(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_channels of the first needs to match in_channels of the second!\n",
      "in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\n"
     ]
    }
   ],
   "source": [
    "# this way, adjacent genes (= 2d_blocks) need to have the correct dimensions\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=1,out_channels=2), Sequential_block_2d(in_channels=3, out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"out_channels of the first needs to match in_channels of the second!\")\n",
    "\n",
    "# but there's more redundancy:\\the first gene needs to have the same number of in_channels as there are colour channels\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=COLOUR_CHANNEL_COUNT + 1,out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brainstorm on How to Encode Individuals\n",
    "We need to talk about this right now because we want to adapt our `NN_individual.blocks_2d` definition according to it.\n",
    "The options are:\n",
    "1. We specify `Sequential_block_2d.in_channels` and `~.out_channels` separately for each individual and only allow concatenation if the criteria are met. This is probably not super clever...\n",
    "2. Genotype-closure: The parameters that are adapted through GP will never leave the space of syntacticly correct indivuals\n",
    "    - The first gene is not allowed to choose `~.in_channels`, it must match `COLOUR_CHANNEL_COUNT`\n",
    "    - Every gene but the first is not allowed to choose `~.in_channels`, it must match `~.out_channels` of the prior gene\n",
    "    - How will this change the gene class `Sequential_block_2d`?\n",
    "        - Set `Sequential_block_2d.in_channels` only programatically, in `NN_individual.__init__()`\n",
    "        - Don't even let the genes inherit from `nn.Module`, only the individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the genetic information of one 2d block\n",
    "class Gene_2d_block:\n",
    "    def __init__(self,\n",
    "                 out_channels: int,\n",
    "                 in_channels: int = None, # set in the individual's constructor\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_stride = conv_stride\n",
    "        self.conv_padding = conv_padding\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.pool_padding = pool_padding\n",
    "\n",
    "    def toString(self, tab_count: int = 0):\n",
    "        indentation = \"\"\n",
    "        for tab in range(tab_count): indentation += f\"\\t\"\n",
    "        return f\"{indentation}out_channels = {self.out_channels}\\n\"+\\\n",
    "        f\"{indentation}conv_2d (kernel, stride, padding) =\\t({self.conv_kernel_size}, {self.conv_stride}, {self.conv_padding})\\n\"+\\\n",
    "        f\"{indentation}max_pool_2d (kernel, stride, padding) =\\t({self.pool_kernel_size}, {self.pool_stride}, {self.pool_padding})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NN_individual(\n",
       "   (blocks_2d): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU()\n",
       "       (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU()\n",
       "       (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "   )\n",
       "   (flatten): Flatten(start_dim=0, end_dim=-1)\n",
       "   (lazyLin): Linear(in_features=343, out_features=10, bias=True)\n",
       " ),\n",
       " tensor([-0.0134, -0.0387, -0.0594,  0.0336,  0.0421,  0.0608, -0.1380, -0.0417,\n",
       "          0.1144,  0.0008], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \n",
    "        super().__init__()\n",
    "        self.name=name # a name for easier tracking inside a GP run\n",
    "        ''' build the full sequential from the gene information (genes_2d_block) '''\n",
    "        self.blocks_2d = nn.Sequential()\n",
    "        # the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "        # the others need to have as in_channels the number of out_channels from the previous block\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            self.blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lazyLin(x)\n",
    "        #print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Code\n",
    "Now we need to decide the genetic stuff.\n",
    "1. Initial Population\n",
    "2. Fitness Measure\n",
    "3. Selection\n",
    "4. Genetic Operators\n",
    "    - Cloning or Crossover\n",
    "    - Mutation\n",
    "    \n",
    "### Hyperparameter-landscape is vast. Here's a list:\n",
    "- Net architecture\n",
    "    - kind of layers, number of layers\n",
    "        - for convolution/pooling: kernel size, stride, padding, (dilation) **[implemented]**\n",
    "    - number of neurons per layer\n",
    "    - activation function for each layer\n",
    "- cost function\n",
    "    - base term (e.g. square cost, log-likelihood, cross-entropy, ect.)\n",
    "    - toppings \n",
    "        - regularization of weights (L2, L1, dropout, etc.)\n",
    "- weights and biases optimization technique (= optimizer)\n",
    "    - SGD (= stochastic gradient descent)\n",
    "    - Hessian technique, i.e. momentum-based descent\n",
    "    - PyTorch's various other (e.g. *Adam* optimizer)\n",
    "- learning parameters\n",
    "    - η ... learning rate\n",
    "        - constant, or epoch-dependent, or accuracy-dependent, or a mix\n",
    "    - \\# of epochs\n",
    "        - constant, or early stopping\n",
    "    - (`mini_batch_size` - this one might be canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Within-One-Gen-Constant Hyperparameters in `NN_individual`\n",
    "We now bake:\n",
    "- the individual-specific, hyperparameters (that don't change within one gen)\n",
    "- the fitness dictionaries\n",
    "\n",
    "into parameters of the `NN_individual` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NN_individual(\n",
       "   (blocks_2d): Sequential()\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (lazyLin): Linear(in_features=784, out_features=10, bias=True)\n",
       "   (loss_fn): CrossEntropyLoss()\n",
       " ),\n",
       " tensor([[ 0.4219,  0.0387,  0.6340,  0.2156,  0.1363,  0.3297,  0.0587,  0.6220,\n",
       "           0.4417, -0.9403]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, \n",
    "                 genes_2d_block: list[Gene_2d_block] = [],  # <- genes for the 2d convolution blocks\n",
    "                 name: str = \"nn0\",                         # <- an inside-population-unique name\n",
    "                 optimizer_gene: str = \"sgd\",               # <- gene for the optimizer\n",
    "                 lr = .1,                                   # <- learning rate\n",
    "                 loss_fn = nn.CrossEntropyLoss(),           # <- loss function\n",
    "                 device = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.blocks_2d = self.__genotype_phenotype_mapping_2d_blocks__(genes_2d_block)\n",
    "        #self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default is: start_dim = 1\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1) # default is: start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "        self.name = name\n",
    "        self.optimizer = self.__genotype_phenotype_mapping_optimizer__(optimizer_gene, lr)\n",
    "        self.lr = lr\n",
    "        self.loss_fn = loss_fn\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.acc = 0\n",
    "        self.running_acc = 0\n",
    "        self.train_losses = {}\n",
    "        self.test_losses = {}\n",
    "        self.accs = {}\n",
    "        \n",
    "    def __genotype_phenotype_mapping_2d_blocks__(self, genes_2d_block: list[Gene_2d_block]):\n",
    "        ''' build 'n return the full sequential from the gene information (genes_2d_block) \n",
    "            the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "            the others need to have as in_channels the number of out_channels from the previous block\n",
    "            there's a nn.Module (Lazy*) that automatically infers the number of in_channels - not used here '''\n",
    "        blocks_2d = nn.Sequential()\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        return blocks_2d\n",
    "    \n",
    "    def __genotype_phenotype_mapping_optimizer__(self, optimizer_gene: str, lr: float):\n",
    "        if optimizer_gene.lower() == \"sgd\":\n",
    "            return torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        return torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lazyLin(x)\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual()#genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize, Visualize, Visualize\n",
    "Create a class called population of which an instance acts as an array of `NN_individual`s with extra functionality that regards the whole population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created by Chat\n",
    "class NN_population:\n",
    "    def __init__(self, individuals: list[NN_individual]):\n",
    "        self.individuals = individuals  # This will hold instances of NN_individual\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.individuals[index]  # Allows pop[i] access\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.individuals)  # Allows len(pop)\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self.individuals[index] = value  # Allows pop[i] = value\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.individuals)  # Allows iteration over pop\n",
    "    \n",
    "    def plot_accs(self, gen = 0):\n",
    "        plt.figure(figsize=(15, 6))  # Set the figure size\n",
    "        # Step 2: Loop through each individual in the population and plot their accuracies\n",
    "        for ind in self.individuals:\n",
    "            x_labels = list(ind.accs.keys())  # Extract the epoch/batch labels (x-axis)\n",
    "            y_values = [float(val.cpu().item()*100) for val in ind.accs.values()]  # Convert tensors to floats\n",
    "            # Step 3: Plot each individual's accuracies\n",
    "            plt.plot(x_labels, y_values, marker='o', linestyle='-', label=ind.name)\n",
    "            \n",
    "        # Step 3: Customize the plot\n",
    "        plt.xlabel('Epoch@Batch')  # Label for the x-axis\n",
    "        plt.ylabel('Accuracy [%]')     # Label for the y-axis\n",
    "        plt.title('Accuracy per Epoch and Batch')  # Title of the plot\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "        plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "        plt.grid(True)  # Show grid\n",
    "        plt.legend()  # Show legend\n",
    "\n",
    "        # Step 4: Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate the image size after a layer has been applied\n",
    "    assume all operations to be x/y symmetric '''\n",
    "def output_size(h_in, kernel_size, stride, padding):\n",
    "    h_out = (h_in + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    #w_out = (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    \n",
    "    return h_out # = w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_random_population(pop_size: int, \n",
    "                             max_2d_block_count: int = 3, \n",
    "                             max_kernel_size: int = 11,\n",
    "                             name_prefix=\"nn\",\n",
    "                             device=\"cpu\",\n",
    "                             print_summary: bool = True) -> NN_population:\n",
    "    population = []\n",
    "    for i in range(pop_size):\n",
    "        genes_2d_block = []\n",
    "        input_image_size = IMAGE_HEIGHT # = IMAGE_WIDTH (assumed)\n",
    "        name=name_prefix+str(i)\n",
    "        if print_summary: print(f\"Individual '{name}' <-- ({input_image_size} x {input_image_size})\")\n",
    "        for j in range(random.randint(1, max_2d_block_count)):\n",
    "            if print_summary: print(f\"\\tBlock {j}\")\n",
    "            conv_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)))\n",
    "            conv_stride=random.randint(1,conv_kernel_size)\n",
    "            conv_padding=random.randint(0,conv_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "            # update input image size after convolutional layer\n",
    "            input_image_size = output_size(input_image_size, conv_kernel_size, conv_stride, conv_padding)\n",
    "            pool_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size)))\n",
    "            pool_stride=max(random.randint(1,pool_kernel_size), random.randint(1,pool_kernel_size))\n",
    "            pool_padding=random.randint(0,pool_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "            block = Gene_2d_block(\n",
    "                out_channels=random.randint(3,15), # not fine-tuned\n",
    "                conv_kernel_size=conv_kernel_size,\n",
    "                conv_padding=conv_padding,\n",
    "                conv_stride=conv_stride,\n",
    "                pool_kernel_size=pool_kernel_size,\n",
    "                pool_padding=pool_padding,\n",
    "                pool_stride=pool_stride\n",
    "            )\n",
    "            genes_2d_block.append(block)\n",
    "            input_image_size = output_size(input_image_size, pool_kernel_size, pool_stride, pool_padding)\n",
    "            if print_summary: print(f\"{block.toString(tab_count=2)} --> ({input_image_size} x {input_image_size})\")\n",
    "        population.append(NN_individual(genes_2d_block=genes_2d_block, name=name, device=device))\n",
    "    return NN_population(population)\n",
    "testPop = create_random_population(pop_size=7, max_2d_block_count=3, print_summary=False)\n",
    "try:\n",
    "    for ind in testPop:\n",
    "        ind.eval()\n",
    "        with torch.inference_mode():\n",
    "            ind(testX)\n",
    "except:\n",
    "    print(\"oh, oh! exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Evaluation\n",
    "We want a population that:\n",
    "- achieves high (validation/test data) accuracy after training\n",
    "    - the final accuracy `acc(NN1(t_final))` of an individual `NN1` is used\n",
    "- trains fast, i.e. takes little CPU time to achieve high accuracy called **Running Accuracy**\n",
    "    - the individual's accuracy `acc(NN1(t))` is summed over given timestamps `t`, like `Σ_t{acc(NN1(t))}`\n",
    "    - possibly we want to value early accuracy more, summing `Σ_t{acc(NN1(t))/t}` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_one_batch(ind: NN_individual, # <- model to be trained in-place\n",
    "                          X, y,               # <- train batch, e.g. X.shape = [32, 1, 28, 28]\n",
    "                          ) -> float:\n",
    "  train_loss = 0\n",
    "  ind.train()\n",
    "  X, y = X.to(ind.device), y.to(ind.device)\n",
    "  y_pred = ind(X)\n",
    "  loss = ind.loss_fn(y_pred, y)\n",
    "  train_loss += loss\n",
    "  ind.optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  ind.optimizer.step()\n",
    "  return train_loss # <- no need to return ind, it is modified in-place\n",
    "\n",
    "def test_model(ind: NN_individual,            # <- model to be tested\n",
    "               test_dl,                       # <- test dataloader (= multiple batches)\n",
    "               ) -> tuple[float, float]:      # -> return (loss_total, acc_total)\n",
    "  loss_total, acc_total = 0, 0\n",
    "  ind.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X, y) in enumerate(test_dl):\n",
    "      X, y = X.to(ind.device), y.to(ind.device)\n",
    "      preds = ind(X)\n",
    "      loss_batch = ind.loss_fn(preds, y)\n",
    "      loss_total += loss_batch\n",
    "      acc_batch = functional.accuracy(preds, y, task=\"multiclass\", num_classes=CLASSIFICATION_CATEGORIES_COUNT)\n",
    "      acc_total += acc_batch\n",
    "\n",
    "    loss_total /= len(test_dl)\n",
    "    acc_total /= len(test_dl)\n",
    "  return (loss_total, acc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the whole population simultaneously, populating the individuals' fitness value parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def train_and_evaluate_gen(pop: NN_population,\n",
    "                           train_dl,  # <- train dataloader\n",
    "                           test_dl,   # <- test dataloader\n",
    "                           testing_interval = 300,      # <- after how many batches should we test an individual\n",
    "                           testing_data_fraction = 1.0, # <- amount of test_dl to be used (1=100% takes a lot of time)\n",
    "                           max_epochs = 5):\n",
    "  # prepare the reduced testing data loader\n",
    "  total_batches = len(test_dl)\n",
    "  num_batches_to_sample = int(total_batches * testing_data_fraction) # number of batches to select\n",
    "  random_batch_indices = random.sample(range(total_batches), num_batches_to_sample) # random indices (without replacement)\n",
    "  test_subset = Subset(test_dl.dataset, random_batch_indices)\n",
    "  test_subset_dl = DataLoader(test_subset, batch_size=test_dl.batch_size, shuffle=False, num_workers=test_dl.num_workers)\n",
    "\n",
    "  # re-initialize pop's fitness values:\n",
    "  for ind in pop:\n",
    "    ind.acc, ind.running_acc = 0, 0\n",
    "    ind.train_losses, ind.test_losses, ind.accs = {}, {}, {}\n",
    "  \n",
    "  # train each individual \"simultaneously\" by making the epoch-loop the outer one\n",
    "  for epoch in range(max_epochs):\n",
    "    print(f\"*** Commencing epoch {epoch+1} / {max_epochs}. ***\")\n",
    "    for i in range(len(pop)):\n",
    "      print(f\"Training {pop[i].name} for {len(train_dl)} batches (of size {train_dl.batch_size}): \", end='')\n",
    "      for batch, (X, y) in tqdm(enumerate(train_dl)):\n",
    "        # update the weights and biases of the NN pop[i]\n",
    "        train_loss = train_model_one_batch(pop[i], X=X, y=y)\n",
    "        if batch % testing_interval == 0: # test the model (every once and while)\n",
    "          test_loss, acc = test_model(pop[i], test_dl=test_subset_dl)\n",
    "          # Print out what's happening\n",
    "          #print(f\"e_{epoch}@b_{batch} | {pop[i].name}: test loss {test_loss:.3f}, accuracy {acc*100:.2f}%\")\n",
    "          # store the values for fitness evaluation (NN_individual.name is assumed a unique identifier here)\n",
    "          pop[i].train_losses[f\"e_{epoch}@b_{batch}\"] = train_loss\n",
    "          pop[i].test_losses[f\"e_{epoch}@b_{batch}\"] = test_loss\n",
    "          pop[i].accs[f\"e_{epoch}@b_{batch}\"] = acc\n",
    "      # here we could select directly, i.e. before the whole train_dl over max_epochs no. of iterations has been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Example Data\n",
    "Use the example dataloaders to populate the fitness values for a generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = create_random_population(pop_size=5, max_2d_block_count=5, max_kernel_size=7, name_prefix=\"gen0.\", device=device, print_summary=False)\n",
    "pop[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Commencing epoch 1 / 1. ***\n",
      "Training gen0.0 for 1875 batches (of size 32): "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [00:19, 95.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gen0.1 for 1875 batches (of size 32): "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1269it [00:14, 97.66it/s] "
     ]
    }
   ],
   "source": [
    "train_and_evaluate_gen(pop, train_dl_f_mnist, test_dl_f_mnist, testing_interval=10, max_epochs=1, testing_data_fraction=.1)\n",
    "pop.plot_accs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
