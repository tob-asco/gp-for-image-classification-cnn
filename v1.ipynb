{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121 running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"{torch.__version__} running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Run for Image Classification\n",
    "Your problem needs to fulfill the following criteria.\n",
    "1. It is an image classification problem.\n",
    "2. You supply marked training images and marked validation images.\n",
    "\n",
    "Within those, the run is flexible and adapts itself to your problem.\n",
    "Now, please describe your images and problem by setting those global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28 # <-- number of width-pixels\n",
    "IMAGE_HEIGHT = 28 # <-- number of height-pixels\n",
    "COLOUR_CHANNEL_COUNT = 1 # <-- RGB images would have 3\n",
    "CLASSIFICATION_CATEGORIES_COUNT = 10 # <-- the amount of possible categories of which each image shall be marked with one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Example Data\n",
    "To check the code, we prepare example data: Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data (and not testing data)\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
    "print(f\"train_data.classes = {train_data.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dl_f_mnist.batch_size = 32\n",
      "len(next(iter(train_dl_f_mnist))) = 2\n",
      "next(iter(train_dl_f_mnist))[0].shape = torch.Size([32, 1, 28, 28])\n",
      "len(train_dl_f_mnist) = 1875, len(test_dl_f_mnist) = 313\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "MINI_BATCH_SIZE = 32 # constant for now\n",
    "\n",
    "# Turn datasets into iterables (batches), shuffeling train data every epoch (test data not)\n",
    "train_dl_f_mnist = DataLoader(train_data, batch_size=MINI_BATCH_SIZE, shuffle=True)\n",
    "test_dl_f_mnist = DataLoader(test_data, batch_size=MINI_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"train_dl_f_mnist.batch_size = {train_dl_f_mnist.batch_size}\") \n",
    "print(f\"len(next(iter(train_dl_f_mnist))) = {len(next(iter(train_dl_f_mnist)))}\") \n",
    "print(f\"next(iter(train_dl_f_mnist))[0].shape = {next(iter(train_dl_f_mnist))[0].shape}\") \n",
    "print(f\"len(train_dl_f_mnist) = {len(train_dl_f_mnist)}, len(test_dl_f_mnist) = {len(test_dl_f_mnist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 2 # index in the batch, 0 .. 31\n",
    "train_features_batch, train_labels_batch = next(iter(train_dl_f_mnist))\n",
    "print(f\"Image shape: {train_features_batch[image_index].shape}\")\n",
    "plt.imshow(train_features_batch[image_index].squeeze(), cmap=\"gray\") # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(str(train_labels_batch[image_index].item())+\" i.e. \"+train_data.classes[train_labels_batch[image_index].item()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Individuals' Class\n",
    "- The Hyperparamters should be passed to the constructor in a way that is both convenient for GP and for PyTorch.\n",
    "- I think I want to define a class for one `nn.Sequential` 2d-block\n",
    "    - All possible instances should be concatenable with all possible instances\n",
    "- Then, an individual is built from the concatenation of many such blocks, plus data preparation and final f.c. 2d_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\\nprint(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")\\n'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded later in the notebook...\n",
    "class Sequential_block_2d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_channels: int, # the number of output neurons after the full block\n",
    "                 in_channels: int = 1, # should not be set here, but is set in Individual's __init__()\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=conv_kernel_size,\n",
    "                      stride=conv_stride,\n",
    "                      padding=conv_padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size,\n",
    "                         stride=pool_stride,\n",
    "                         padding=pool_padding)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# generate some testing blocks (TODO: write actual unit tests)\n",
    "testBlock1 = Sequential_block_2d(in_channels=1,out_channels=5)\n",
    "testBlock2 = Sequential_block_2d(in_channels=5,out_channels=3)\n",
    "torch.manual_seed(42)\n",
    "'''\n",
    "testX = torch.randn(COLOUR_CHANNEL_COUNT,IMAGE_WIDTH,IMAGE_HEIGHT).to(device)\n",
    "'''\n",
    "print(f\"testX.shape = {testX.shape}\\ntestX[:,:3]: {testX[:,:3]}\")\n",
    "print(f\"testBlock1(testX).shape = {testBlock1(testX).shape}\\ntestBlock1(testX)[:1,:3]: {testBlock1(testX)[:1,:3]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' superseded later in the notebook...\\n# class for image classification individuals\\nclass NN_individual(nn.Module):\\n    def __init__(self, blocks_2d: list[Sequential_block_2d],\\n                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\\n                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\\n                 fin_res: int = 5): # output dimension of the final max pooling \\n                                    # producing size (fin_res * fin_res)\\n        super().__init__()\\n        self.blocks_2d = blocks_2d\\n        # add a final max pool\\n        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\\n        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\\n        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\\n        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\\n                      out_features = out_dimensions)\\n    def forward(self, x):\\n        for i in range(len(self.blocks_2d)):\\n            x = self.blocks_2d[i](x)\\n        x = self.last_max_pool_adaptive(x)\\n        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\\n        x = self.flatten(x)\\n        print(f\"flatten output shape is {x.shape}\")\\n        x = self.lin(x)\\n        print(f\"lin output shape is {x.shape}\")\\n        return x\\n    \\ntestIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\\ntestIndividual(testX)\\n'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded later in the notebook...\n",
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, blocks_2d: list[Sequential_block_2d],\n",
    "                 in_dimensions: int = COLOUR_CHANNEL_COUNT,\n",
    "                 out_dimensions: int = CLASSIFICATION_CATEGORIES_COUNT,\n",
    "                 fin_res: int = 5): # output dimension of the final max pooling \n",
    "                                    # producing size (fin_res * fin_res)\n",
    "        super().__init__()\n",
    "        self.blocks_2d = blocks_2d\n",
    "        # add a final max pool\n",
    "        # Why? Because then torch handles the dimensions through the \"adaptive\"ness\n",
    "        self.last_max_pool_adaptive = nn.AdaptiveAvgPool2d((fin_res, fin_res))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lin = nn.Linear(in_features = fin_res * fin_res * blocks_2d[-1].out_channels,\n",
    "                      out_features = out_dimensions)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.last_max_pool_adaptive(x)\n",
    "        print(f\"last_max_pool_adaptive output shape is {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lin(x)\n",
    "        print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(blocks_2d=[testBlock1, testBlock2])\n",
    "testIndividual(testX)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' superseded\\n# this way, adjacent genes (= 2d_blocks) need to have the correct dimensions\\n# e.g. the following will error:\\nbadIndividual = NN_individual([Sequential_block_2d(in_channels=1,out_channels=2), Sequential_block_2d(in_channels=3, out_channels=5)])\\ntry:\\n    print(badIndividual(testX))\\nexcept:\\n    print(\"out_channels of the first needs to match in_channels of the second!\")\\n\\n# but there\\'s more redundancy:\\the first gene needs to have the same number of in_channels as there are colour channels\\n# e.g. the following will error:\\nbadIndividual = NN_individual([Sequential_block_2d(in_channels=COLOUR_CHANNEL_COUNT + 1,out_channels=5)])\\ntry:\\n    print(badIndividual(testX))\\nexcept:\\n    print(\"in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\")\\n'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' superseded\n",
    "# this way, adjacent genes (= 2d_blocks) need to have the correct dimensions\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=1,out_channels=2), Sequential_block_2d(in_channels=3, out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"out_channels of the first needs to match in_channels of the second!\")\n",
    "\n",
    "# but there's more redundancy:\\the first gene needs to have the same number of in_channels as there are colour channels\n",
    "# e.g. the following will error:\n",
    "badIndividual = NN_individual([Sequential_block_2d(in_channels=COLOUR_CHANNEL_COUNT + 1,out_channels=5)])\n",
    "try:\n",
    "    print(badIndividual(testX))\n",
    "except:\n",
    "    print(\"in_channels of the first gene needs to match the COLOUR_CHANNEL_COUNT of the problem!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brainstorm on How to Encode Individuals\n",
    "We need to talk about this right now because we want to adapt our `NN_individual.blocks_2d` definition according to it.\n",
    "The options are:\n",
    "1. We specify `Sequential_block_2d.in_channels` and `~.out_channels` separately for each individual and only allow concatenation if the criteria are met. This is probably not super clever...\n",
    "2. Genotype-closure: The parameters that are adapted through GP will never leave the space of syntacticly correct indivuals\n",
    "    - The first gene is not allowed to choose `~.in_channels`, it must match `COLOUR_CHANNEL_COUNT`\n",
    "    - Every gene but the first is not allowed to choose `~.in_channels`, it must match `~.out_channels` of the prior gene\n",
    "    - How will this change the gene class `Sequential_block_2d`?\n",
    "        - Set `Sequential_block_2d.in_channels` only programatically, in `NN_individual.__init__()`\n",
    "        - Don't even let the genes inherit from `nn.Module`, only the individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_channels = 3 <-- (28 x 28)\n",
      "conv_2d (kernel, stride, padding) =\t(3, 1, 1) --> (28 x 28)\n",
      "max_pool_2d (kernel, stride, padding) =\t(2, 2, 0) --> (14 x 14)\n"
     ]
    }
   ],
   "source": [
    "''' Calculate the image size after a layer has been applied\n",
    "    assume all operations to be x/y symmetric '''\n",
    "def output_size(h_in, kernel_size, stride, padding):\n",
    "    h_out = (h_in + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    #w_out = (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    return h_out # = w_out\n",
    "\n",
    "# class for the genetic information of one 2d block\n",
    "class Gene_2d_block:\n",
    "    def __init__(self,\n",
    "                 input_image_size: int,\n",
    "                 out_channels: int,\n",
    "                 conv_kernel_size: int = 3,\n",
    "                 conv_stride: int = 1,\n",
    "                 conv_padding: int = 1,\n",
    "                 pool_kernel_size: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 pool_padding: int = 0):\n",
    "        self.input_image_size = input_image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = None\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_stride = conv_stride\n",
    "        self.conv_padding = conv_padding\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.pool_padding = pool_padding\n",
    "\n",
    "        self.after_conv_image_size = output_size(input_image_size, conv_kernel_size, conv_stride, conv_padding)\n",
    "        self.output_image_size = output_size(self.after_conv_image_size, pool_kernel_size, pool_stride, pool_padding)\n",
    "\n",
    "    def toString(self, tab_count: int = 0):\n",
    "        indentation = \"\"\n",
    "        for tab in range(tab_count): indentation += f\"\\t\"\n",
    "        return f\"{indentation}out_channels = {self.out_channels} <-- ({self.input_image_size} x {self.input_image_size})\\n\"+\\\n",
    "        f\"{indentation}conv_2d (kernel, stride, padding) =\\t({self.conv_kernel_size}, {self.conv_stride}, {self.conv_padding}) --> ({self.after_conv_image_size} x {self.after_conv_image_size})\\n\"+\\\n",
    "        f\"{indentation}max_pool_2d (kernel, stride, padding) =\\t({self.pool_kernel_size}, {self.pool_stride}, {self.pool_padding}) --> ({self.output_image_size} x {self.output_image_size})\"\n",
    "\n",
    "print(Gene_2d_block(IMAGE_WIDTH, 3).toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this is supserseded below...\\n# class for image classification individuals\\nclass NN_individual(nn.Module):\\n    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \\n        super().__init__()\\n        self.name=name # a name for easier tracking inside a GP run\\n        # build the full sequential from the gene information (genes_2d_block)\\n        self.blocks_2d = nn.Sequential()\\n        # the first 2d_block needs to have as many in_channels as there are colour channels\\n        # the others need to have as in_channels the number of out_channels from the previous block\\n        for i in range(len(genes_2d_block)):\\n            if i == 0:\\n                in_channels = COLOUR_CHANNEL_COUNT\\n            else:\\n                in_channels = genes_2d_block[i-1].out_channels\\n            self.blocks_2d.append(nn.Sequential(\\n                nn.Conv2d(in_channels=in_channels,\\n                    out_channels=genes_2d_block[i].out_channels,\\n                    kernel_size=genes_2d_block[i].conv_kernel_size,\\n                    stride=genes_2d_block[i].conv_stride,\\n                    padding=genes_2d_block[i].conv_padding),\\n                nn.ReLU(),\\n                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\\n                    stride=genes_2d_block[i].pool_stride,\\n                    padding=genes_2d_block[i].pool_padding)))\\n        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\\n        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\\n    def forward(self, x):\\n        for i in range(len(self.blocks_2d)):\\n            x = self.blocks_2d[i](x)\\n        x = self.flatten(x)\\n        #print(f\"flatten output shape is {x.shape}\")\\n        x = self.lazyLin(x)\\n        #print(f\"lin output shape is {x.shape}\")\\n        return x\\n    \\ntestIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\\ntestIndividual, testIndividual(testX)\\n'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' this is supserseded below...\n",
    "# class for image classification individuals\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, genes_2d_block: list[Gene_2d_block], name=\"nn0\"): \n",
    "        super().__init__()\n",
    "        self.name=name # a name for easier tracking inside a GP run\n",
    "        # build the full sequential from the gene information (genes_2d_block)\n",
    "        self.blocks_2d = nn.Sequential()\n",
    "        # the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "        # the others need to have as in_channels the number of out_channels from the previous block\n",
    "        for i in range(len(genes_2d_block)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = genes_2d_block[i-1].out_channels\n",
    "            self.blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=genes_2d_block[i].out_channels,\n",
    "                    kernel_size=genes_2d_block[i].conv_kernel_size,\n",
    "                    stride=genes_2d_block[i].conv_stride,\n",
    "                    padding=genes_2d_block[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=genes_2d_block[i].pool_kernel_size,\n",
    "                    stride=genes_2d_block[i].pool_stride,\n",
    "                    padding=genes_2d_block[i].pool_padding)))\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1) # default start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"flatten output shape is {x.shape}\")\n",
    "        x = self.lazyLin(x)\n",
    "        #print(f\"lin output shape is {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual(genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Code\n",
    "Now we need to decide the genetic stuff.\n",
    "1. Initial Population\n",
    "2. Fitness Measure\n",
    "3. Selection\n",
    "4. Genetic Operators\n",
    "    - Cloning or Crossover\n",
    "    - Mutation\n",
    "    \n",
    "### Hyperparameter-landscape is vast. Here's a list:\n",
    "- Net architecture\n",
    "    - kind of layers, number of layers\n",
    "        - for convolution/pooling: kernel size, stride, padding, (dilation) **[implemented]**\n",
    "    - number of neurons per layer\n",
    "    - activation function for each layer\n",
    "- cost function\n",
    "    - base term (e.g. square cost, log-likelihood, cross-entropy, ect.)\n",
    "    - toppings \n",
    "        - regularization of weights (L2, L1, dropout, etc.)\n",
    "- weights and biases optimization technique (= optimizer)\n",
    "    - SGD (= stochastic gradient descent)\n",
    "    - Hessian technique, i.e. momentum-based descent\n",
    "    - PyTorch's various other (e.g. *Adam* optimizer)\n",
    "- learning parameters\n",
    "    - Î· ... learning rate\n",
    "        - constant, or epoch-dependent, or accuracy-dependent, or a mix\n",
    "    - \\# of epochs\n",
    "        - constant, or early stopping\n",
    "    - (`mini_batch_size` - this one might be canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Within-One-Gen-Constant Hyperparameters in `NN_individual`\n",
    "We now bake:\n",
    "- the individual-specific, hyperparameters (that don't change within one gen)\n",
    "- the fitness dictionaries\n",
    "\n",
    "into parameters of the `NN_individual` class.\n",
    "\n",
    "We construct this class from a `NN_dna` class that contains all the genetic information necessary to build the `NN_individual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class containing various genes (with their vanilla values) and the G2P mappings\n",
    "class NN_dna():\n",
    "    def __init__(self,\n",
    "                 blocks_2d: list[Gene_2d_block] = [],\n",
    "                 optimizer: int = 0,\n",
    "                 lr: float = .1,\n",
    "                 loss_fn: int = 0,\n",
    "                 ) -> None:\n",
    "        self.blocks_2d_gene = blocks_2d\n",
    "        self.optimizer_gene = optimizer\n",
    "        self.lr = lr\n",
    "        self.loss_fn_gene = loss_fn\n",
    "    \n",
    "    #### Genotype To Phenotype Mappings (G2P) ####\n",
    "    def blocks_2d_G2P(self):\n",
    "        ''' build 'n return the full sequential from the gene information (genes_2d_block) \n",
    "            the first 2d_block needs to have as many in_channels as there are colour channels\n",
    "            the others need to have as in_channels the number of out_channels from the previous block\n",
    "            there's a nn.Module (Lazy*) that automatically infers the number of in_channels - not used here '''\n",
    "        blocks_2d = nn.Sequential()\n",
    "        for i in range(len(self.blocks_2d_gene)):\n",
    "            if i == 0:\n",
    "                in_channels = COLOUR_CHANNEL_COUNT\n",
    "            else:\n",
    "                in_channels = self.blocks_2d_gene[i-1].out_channels\n",
    "            blocks_2d.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=self.blocks_2d_gene[i].out_channels,\n",
    "                    kernel_size=self.blocks_2d_gene[i].conv_kernel_size,\n",
    "                    stride=self.blocks_2d_gene[i].conv_stride,\n",
    "                    padding=self.blocks_2d_gene[i].conv_padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.blocks_2d_gene[i].pool_kernel_size,\n",
    "                    stride=self.blocks_2d_gene[i].pool_stride,\n",
    "                    padding=self.blocks_2d_gene[i].pool_padding)))\n",
    "        return blocks_2d\n",
    "        \n",
    "    optimizer_dict = {0: \"SGD\", 1: \"adam\"}\n",
    "    def optimizer_G2P(self, model_parameters):\n",
    "        if self.optimizer_gene == 0: return torch.optim.SGD(model_parameters, lr=self.lr)\n",
    "        if self.optimizer_gene == 1: return torch.optim.Adam(model_parameters, lr=self.lr)\n",
    "        raise ValueError(f\"'{self.optimizer_gene}' is not a gene for which we have an optimizer encoded.\")\n",
    "    \n",
    "    loss_fn_dict = {0: \"CrossEn\", 1: \"L1\", 2: \"Huber\"}\n",
    "    def loss_fn_G2P(self):\n",
    "        if self.loss_fn_gene == 0: return nn.CrossEntropyLoss()\n",
    "        if self.loss_fn_gene == 1: return nn.L1Loss()\n",
    "        if self.loss_fn_gene == 2: return nn.HuberLoss()\n",
    "        raise ValueError(f\"'{self.loss_fn_gene}' is not a gene for which we have a loss function encoded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NN_individual(\n",
       "   (blocks_2d): Sequential()\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (lazyLin): Linear(in_features=784, out_features=10, bias=True)\n",
       "   (loss_fn): CrossEntropyLoss()\n",
       " ),\n",
       " tensor([[-0.5065, -0.0216,  0.2096,  0.9886, -0.4051, -0.7625, -0.8564, -0.0976,\n",
       "          -0.5313, -0.0121]], device='cuda:0', grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' class for image classification individuals\n",
    "    Essentially, it converts NN_dna (provided to __init__)\n",
    "    into a working NN with a forward method\n",
    "'''\n",
    "class NN_individual(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dna: NN_dna = NN_dna(),    # <- contains all genes\n",
    "                 name: str = \"nn0\",         # <- name unique within a population/generation\n",
    "                 device = device):\n",
    "        super().__init__()\n",
    "        self.dna = dna\n",
    "        self.blocks_2d = dna.blocks_2d_G2P()\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1) # default is: start_dim = 1\n",
    "        self.lazyLin = nn.LazyLinear(out_features = CLASSIFICATION_CATEGORIES_COUNT) # automatically infers the number of channels\n",
    "        self.name = name\n",
    "        self.lr = dna.lr\n",
    "        self.optimizer = dna.optimizer_G2P(self.parameters())\n",
    "        self.loss_fn = dna.loss_fn_G2P()\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.acc = 0\n",
    "        self.running_acc = 0\n",
    "        self.train_losses = {}\n",
    "        self.test_losses = {}\n",
    "        self.accs = {}\n",
    "        self.elapsed_training_time = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks_2d)):\n",
    "            x = self.blocks_2d[i](x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lazyLin(x)\n",
    "        return x\n",
    "    \n",
    "testIndividual = NN_individual()#genes_2d_block=[Gene_2d_block(out_channels=4), Gene_2d_block(out_channels=7)])\n",
    "testIndividual, testIndividual(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize, Visualize, Visualize\n",
    "Create a class called population of which an instance acts as an array of `NN_individual`s with extra functionality that regards the whole population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created by Chat\n",
    "class NN_population:\n",
    "    ### Magic Methods ###\n",
    "    def __init__(self, individuals: list[NN_individual]): self.individuals = individuals\n",
    "    def __getitem__(self, index): return self.individuals[index]  # magic pop[i] access\n",
    "    def __len__(self): return len(self.individuals)  # magic len(pop)\n",
    "    def __setitem__(self, index, value): self.individuals[index] = value  # magic pop[i] = value\n",
    "    def __iter__(self): return iter(self.individuals)  # magic for-iterations\n",
    "    \n",
    "    def plot_accs(self, elapsed_time = 0):\n",
    "        plt.figure(figsize=(15, 6))  # Set the figure size\n",
    "        for ind in self.individuals:\n",
    "            x = list(ind.accs.keys())  # Extract the epoch/batch labels (x-axis)\n",
    "            y = [float(val.cpu().item()*100) for val in ind.accs.values()]  # Convert tensors to floats\n",
    "            # Plot each individual's accuracies\n",
    "            plt.plot(x, y, marker='o', linestyle='-', label=f\"{ind.name} ({ind.elapsed_training_time:.1f}s, running acc {ind.running_acc:.2f})\")\n",
    "        plt.xlabel('Epoch@Batch')  # Label for the x-axis\n",
    "        plt.ylabel('Accuracy [%]')     # Label for the y-axis\n",
    "        extra_title = \"\" if elapsed_time == 0 else f\" (took {elapsed_time:.2f}s)\"\n",
    "        plt.title('Accuracy per Epoch and Batch' + extra_title)  # Title of the plot\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "        plt.grid(True)  # Show grid\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1)) # legend on the right\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust plot area size to leave space for the legend\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh, oh! exception\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def create_random_2d_block(input_image_size, max_kernel_size: int) -> Gene_2d_block:\n",
    "    conv_kernel_size=min(random.randint(1,min(input_image_size, max_kernel_size)), random.randint(1,min(input_image_size, max_kernel_size))) # kernel must be smaller than image size!\n",
    "    conv_stride=random.randint(1,conv_kernel_size)\n",
    "    conv_padding=random.randint(0,conv_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "    after_conv_i_s = output_size(input_image_size, conv_kernel_size, conv_stride, conv_padding)\n",
    "    pool_kernel_size=min(random.randint(1,min(after_conv_i_s, max_kernel_size)), random.randint(1,min(after_conv_i_s, max_kernel_size)), random.randint(1,min(after_conv_i_s, max_kernel_size)))\n",
    "    if conv_kernel_size == 0 or pool_kernel_size == 0:\n",
    "        print(\"Exception! A kernel size is 0, which is not allowed.\")\n",
    "        print(f\"input_image_size {input_image_size}, max_kernel_size {max_kernel_size}, conv_kernel_size {conv_kernel_size}, conv_stride {conv_stride}, conv_padding {conv_padding}, after_conv_image_size {after_conv_i_s}, pool_kernel_size {pool_kernel_size}\")\n",
    "    pool_stride=max(random.randint(1,pool_kernel_size), random.randint(1,pool_kernel_size))\n",
    "    pool_padding=random.randint(0,pool_kernel_size//2) # PyTorch: \"pad should be at most half of effective kernel size\"\n",
    "    return Gene_2d_block(\n",
    "        input_image_size=input_image_size,\n",
    "        out_channels=random.randint(3,15), # not fine-tuned\n",
    "        conv_kernel_size=conv_kernel_size,\n",
    "        conv_padding=conv_padding,\n",
    "        conv_stride=conv_stride,\n",
    "        pool_kernel_size=pool_kernel_size,\n",
    "        pool_padding=pool_padding,\n",
    "        pool_stride=pool_stride\n",
    "    )\n",
    "\n",
    "def update_and_check_2d_block_stack(gene_2d_blocks: list[Gene_2d_block]):\n",
    "    protocol = \"\"\n",
    "    if len(gene_2d_blocks) == 0: return \"no block in the stack\"\n",
    "    if gene_2d_blocks[0].input_image_size != IMAGE_HEIGHT:\n",
    "        gene_2d_blocks[0].input_image_size != IMAGE_HEIGHT\n",
    "        protocol += f\"block[0]'s input_image_size was set to IMAGE_HEIGHT ({IMAGE_HEIGHT}), \" \n",
    "    for i, block in enumerate(gene_2d_blocks):\n",
    "        if i > 0 and block.input_image_size != gene_2d_blocks[i-1].output_image_size:\n",
    "            block.input_image_size = gene_2d_blocks[i-1].output_image_size\n",
    "            protocol += f\"block[{i}]'s input_image_size was set to block[{i-1}]'s output_image_size ({block.input_image_size}), \"\n",
    "            block.after_conv_image_size = output_size(block.input_image_size, block.conv_kernel_size, block.conv_stride, block.conv_padding) # update the effective image size after convolution\n",
    "        if block.input_image_size < block.conv_kernel_size:\n",
    "            block.conv_kernel_size = block.input_image_size\n",
    "            protocol += f\"block[{i}]'s conv_kernel_size was decreased to input_image_size ({block.conv_kernel_size}), \"\n",
    "        if block.conv_padding > block.conv_kernel_size // 2:\n",
    "            block.conv_padding = block.conv_kernel_size // 2\n",
    "            protocol += f\"block[{i}]'s conv_padding was decreased to conv_kernel_size//2 ({block.conv_kernel_size // 2}), \"\n",
    "        if block.after_conv_image_size < block.pool_kernel_size:\n",
    "            block.pool_kernel_size = block.after_conv_image_size\n",
    "            protocol += f\"block[{i}]'s pool_kernel_size was decreased to after_conv_size ({block.pool_kernel_size}), \"\n",
    "        if block.pool_padding > block.pool_kernel_size // 2:\n",
    "            block.pool_padding = block.pool_kernel_size // 2\n",
    "            protocol += f\"block[{i}]'s pool_padding was decreased to pool_kernel_size//2 ({block.pool_kernel_size // 2}), \"\n",
    "    return protocol\n",
    "    \n",
    "def create_random_population(pop_size: int, \n",
    "                             max_2d_block_count: int = 3, \n",
    "                             max_kernel_size: int = 11,\n",
    "                             name_prefix=\"nn\",\n",
    "                             device=\"cpu\",\n",
    "                             print_summary: bool = True) -> NN_population:\n",
    "    population = []\n",
    "    for i in range(pop_size):\n",
    "        genes_2d_block = []\n",
    "        input_image_size = IMAGE_HEIGHT # = IMAGE_WIDTH (assumed)\n",
    "        name=name_prefix+str(i)\n",
    "        if print_summary: print(f\"Individual '{name}' <-- ({input_image_size} x {input_image_size})\")\n",
    "        for j in range(random.randint(1, max_2d_block_count)):\n",
    "            if print_summary: print(f\"\\tBlock {j}\")\n",
    "            # create a random conv-pool block and store the corresponding new input_image_size for the block thereafter\n",
    "            block = create_random_2d_block(input_image_size, max_kernel_size)\n",
    "            input_image_size = block.output_image_size\n",
    "            genes_2d_block.append(block)\n",
    "            if print_summary: print(f\"{block.toString(tab_count=2)} --> ({input_image_size} x {input_image_size})\")\n",
    "        dna = NN_dna(blocks_2d=genes_2d_block,\n",
    "                    # here you can change the remaining hyperparameters\n",
    "                    )\n",
    "        population.append(NN_individual(dna=dna, name=name, device=device))\n",
    "    return NN_population(population)\n",
    "testPop = create_random_population(pop_size=7, max_2d_block_count=3, print_summary=False)\n",
    "try:\n",
    "    for ind in testPop:\n",
    "        ind.eval()\n",
    "        with torch.inference_mode():\n",
    "            ind(testX)\n",
    "except:\n",
    "    print(\"oh, oh! exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Evaluation\n",
    "We want a population that:\n",
    "- achieves high (validation/test data) accuracy after training\n",
    "    - the final accuracy `acc(NN1(t_final))` of an individual `NN1` is used\n",
    "- trains fast, i.e. takes little CPU time to achieve high accuracy called **Running Accuracy**\n",
    "    - the individual's accuracy `acc(NN1(t))` is summed over given timestamps `t`, like `Î£_t{acc(NN1(t))}`\n",
    "    - possibly we want to value early accuracy more, summing `Î£_t{acc(NN1(t))/t}` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import functional\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_one_batch(ind: NN_individual, # <- model to be trained in-place\n",
    "                          X, y,               # <- train batch, e.g. X.shape = [32, 1, 28, 28]\n",
    "                          ) -> tuple[float, float]:\n",
    "  train_loss = 0\n",
    "  start_time = time.perf_counter() # Start timing\n",
    "  ind.train()\n",
    "  X, y = X.to(ind.device), y.to(ind.device)\n",
    "  y_pred = ind(X)\n",
    "  loss = ind.loss_fn(y_pred, y)\n",
    "  train_loss += loss\n",
    "  ind.optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  ind.optimizer.step()\n",
    "  end_time = time.perf_counter() # Stop timing\n",
    "  return (train_loss, end_time-start_time)\n",
    "\n",
    "def test_model(ind: NN_individual,            # <- model to be tested\n",
    "               test_dl,                       # <- test dataloader (= multiple batches)\n",
    "               ) -> tuple[float, float]:      # -> return (loss_total, acc_total)\n",
    "  loss_total, acc_total = 0, 0\n",
    "  ind.eval()\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X, y) in enumerate(test_dl):\n",
    "      X, y = X.to(ind.device), y.to(ind.device)\n",
    "      preds = ind(X)\n",
    "      loss_batch = ind.loss_fn(preds, y)\n",
    "      loss_total += loss_batch\n",
    "      acc_batch = functional.accuracy(preds, y, task=\"multiclass\", num_classes=CLASSIFICATION_CATEGORIES_COUNT)\n",
    "      acc_total += acc_batch\n",
    "\n",
    "    loss_total /= len(test_dl)\n",
    "    acc_total /= len(test_dl)\n",
    "  return (loss_total, acc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the whole population simultaneously, populating the individuals' fitness value parameters:\n",
    "- `train_losses` ... a dictionary filled with the train loss function results for each batch (independent of how often we test, because it comes for free)\n",
    "- `test_losses` ... same as above but evaluating on test data instead, and only whenever we choose to test (obviously; this is not free)\n",
    "- `accs` ... a dictionary with same keys as `test_losses`, filled with the fraction of correct model predictions by total number of predictions\n",
    "- `acc` ... a single number - the most recent accuracy (defined similarly as `accs`)\n",
    "- `running_acc` ... a single number - the sum of all known accuracies (i.e. at all times where we tested), divided by the time\n",
    "    - here, we exclude the first accuracy because the division is very big in that case, and accuracy only depends mainly on weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def train_and_evaluate_gen(pop: NN_population,\n",
    "                           train_dl,  # <- train dataloader\n",
    "                           test_dl,   # <- test dataloader\n",
    "                           testing_interval = 300,      # <- after how many batches should we test an individual\n",
    "                           testing_data_fraction = 1.0, # <- amount of test_dl to be used (1=100% takes a lot of time)\n",
    "                           epochs = 5,\n",
    "                           live_plot = True,\n",
    "                           only_last_plot = False,\n",
    "                           no_plot = False):\n",
    "  start_time = time.perf_counter() # Start timing\n",
    "\n",
    "  # prepare the reduced testing data loader\n",
    "  total_batches = len(test_dl)\n",
    "  num_batches_to_sample = int(total_batches * testing_data_fraction) # number of batches to select\n",
    "  random_batch_indices = random.sample(range(total_batches), num_batches_to_sample) # random indices (without replacement)\n",
    "  test_subset = Subset(test_dl.dataset, random_batch_indices)\n",
    "  test_subset_dl = DataLoader(test_subset, batch_size=test_dl.batch_size, shuffle=False, num_workers=test_dl.num_workers)\n",
    "\n",
    "  # re-initialize pop's fitness values:\n",
    "  for ind in pop:\n",
    "    ind.acc, ind.running_acc = 0, 0\n",
    "    ind.train_losses, ind.test_losses, ind.accs = {}, {}, {}\n",
    "  \n",
    "  # train each individual \"simultaneously\" by making the epoch-loop the outer one\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"*** Commencing epoch {epoch+1} / {epochs} for {len(pop)} individuals, one line each. ***\")\n",
    "    for i in range(len(pop)):\n",
    "      #print(f\"Training {pop[i].name} for {len(train_dl)} batches (of size {train_dl.batch_size}): \", end='')\n",
    "      for batch, (X, y) in tqdm(enumerate(train_dl),desc=f\"{i+1}. {pop[i].name}\"):\n",
    "        # train the model (update the weights and biases of the NN pop[i])\n",
    "        pop[i].train_losses[f\"e_{epoch}@b_{batch}\"], elapsed_batch_training_time = train_model_one_batch(pop[i], X=X, y=y)\n",
    "        pop[i].elapsed_training_time += elapsed_batch_training_time\n",
    "        if batch % testing_interval == 0: \n",
    "          # test the model and store the results\n",
    "          pop[i].test_losses[f\"e_{epoch}@b_{batch}\"], pop[i].accs[f\"e_{epoch}@b_{batch}\"] = test_model(pop[i], test_dl=test_subset_dl)\n",
    "          if batch != 0: # don't use the start/benchmark test as this depends mostly on luck of weight initialization\n",
    "            pop[i].running_acc += pop[i].accs[f\"e_{epoch}@b_{batch}\"] / pop[i].elapsed_training_time\n",
    "          if live_plot and not only_last_plot and not no_plot:\n",
    "            clear_output(wait=True)\n",
    "            pop.plot_accs(time.perf_counter() - start_time)\n",
    "      pop[i].test_losses[f\"e_{epoch}@end\"], pop[i].accs[f\"e_{epoch}@end\"] = test_model(pop[i], test_dl=test_dl) # latest precise values\n",
    "      pop[i].acc = pop[i].accs[f\"e_{epoch}@end\"] # store the very last known accuracy\n",
    "      if not live_plot and not only_last_plot and not no_plot:\n",
    "        clear_output(wait=True)\n",
    "        pop.plot_accs(time.perf_counter() - start_time)\n",
    "    # here we could select directly, i.e. before the whole train_dl over max_epochs no. of iterations has been trained\n",
    "\n",
    "  if not no_plot and not only_last_plot:\n",
    "    clear_output(wait=True)\n",
    "    pop.plot_accs(time.perf_counter() - start_time)\n",
    "  elif not no_plot and only_last_plot:\n",
    "    pop.plot_accs(time.perf_counter() - start_time)\n",
    "  else:  \n",
    "    print(f\"This took {time.perf_counter() - start_time:.2f}s.\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Example Data\n",
    "Use the example dataloaders to populate the fitness values for a generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = create_random_population(pop_size=4, max_2d_block_count=4, max_kernel_size=7, name_prefix=\"gen0.\", device=device, print_summary=False)\n",
    "pop[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94464c158f45406cbfeb5cc4e86d8b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('a', 'b', 'c'), value='a'), Dropdown(description='y',â¦"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "%matplotlib inline\n",
    "\n",
    "columns=['a','b','c']\n",
    "data = np.cumsum(np.random.rand(10,3),axis=1)\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "\n",
    "def g(x,y):\n",
    "    plt.scatter(df[x], df[y])\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(g, x=columns, y=columns)\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_and_evaluate_gen(pop, train_dl_f_mnist, test_dl_f_mnist, testing_interval=200, epochs=1, testing_data_fraction=.1, live_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutate\n",
    "Should be guided by randomness, and not too radical.\n",
    "1. Fix a bound of *radicalities* fixed at 1.\n",
    "2. Each possible operation of mutation needs to come with a *factor of impact* **Î»**\n",
    "    - e.g. delete/add 2d block: **Î» = 5**\n",
    "    - e.g. learning rate times 1.1: **Î» = 0.5**\n",
    "3. Now we spin the wheels for each call of `mutate_individual`:\n",
    "    - random radicality **0 < R < 1**\n",
    "    - random raw likelihood **0 < p < 1** for each possible operation\n",
    "    - execute an operation iff **Î»â¢p < R** where **Î»** and **p** correspond to the operation in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_KERNEL_SIZE = 11\n",
    "class Mutation():\n",
    "    def __init__(self):\n",
    "        self.R = random.random() # <- radicality of this mutation instance\n",
    "\n",
    "    ### factors of impact ###\n",
    "    impact_lr_factor_1_1=.3\n",
    "    impact_lr_factor_10=3\n",
    "    impact_add_neuron_to_2d_block=1\n",
    "    impact_delete_neuron_from_2d_block=1\n",
    "    impact_add_2d_block=5\n",
    "    impact_delete_2d_block=5\n",
    "    impact_increase_kernel=.8\n",
    "    impact_decrease_kernel=.8\n",
    "    impact_change_optimizer=10\n",
    "    impact_change_loss_fn=10\n",
    "\n",
    "    ### possible operations (p_raw = 0 will happen most likely, p_raw = 1 least likely, p_raw = -1 definitely) ###\n",
    "    def lr_factor_1_1(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_lr_factor_1_1 < self.R:\n",
    "            if random.random() > .5:\n",
    "                dna.lr *= 1.1\n",
    "                return \"multiplied lr by 1.1\"\n",
    "            else:\n",
    "                dna.lr /= 1.1\n",
    "                return \"divided lr by 1.1\"\n",
    "    def lr_factor_10(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_lr_factor_10 < self.R:\n",
    "            if random.random() > .5:\n",
    "                dna.lr *= 10\n",
    "                return \"multiplied lr by 10\"\n",
    "            else:\n",
    "                dna.lr /= 10\n",
    "                return \"divided lr by 10\"\n",
    "    def add_neuron_to_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_add_neuron_to_2d_block < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            dna.blocks_2d_gene[layer_nr].out_channels += 1\n",
    "            return f\"added neuron to 2d block no. {layer_nr}\"\n",
    "    def delete_neuron_from_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_delete_neuron_from_2d_block < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            if dna.blocks_2d_gene[layer_nr].out_channels > 1: dna.blocks_2d_gene[layer_nr].out_channels -= 1\n",
    "            return f\"deleted neuron from 2d block no. {layer_nr}\"\n",
    "    def add_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_add_2d_block < self.R:\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene) + 1)\n",
    "            input_image_size = IMAGE_WIDTH if layer_nr == 0 else dna.blocks_2d_gene[layer_nr-1].output_image_size\n",
    "            dna.blocks_2d_gene.insert(layer_nr, create_random_2d_block(input_image_size, MAX_KERNEL_SIZE))\n",
    "            # check whether this insertion \"killed\" the entity (and if yes: repair it)\n",
    "            protocol = update_and_check_2d_block_stack(dna.blocks_2d_gene)\n",
    "            return f\"added 2d block at {layer_nr} + {protocol}\"\n",
    "    def delete_2d_block(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_delete_2d_block < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            dna.blocks_2d_gene.pop(layer_nr)\n",
    "            # check whether this deletion \"killed\" the entity (and if yes: repair it)\n",
    "            protocol = update_and_check_2d_block_stack(dna.blocks_2d_gene)\n",
    "            return f\"deleted 2d block at {layer_nr} + {protocol}\"\n",
    "    def increase_kernel(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_increase_kernel < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            if random.random() > .5:\n",
    "                # check whether the kernel may be increased\n",
    "                if dna.blocks_2d_gene[layer_nr].conv_kernel_size < dna.blocks_2d_gene[layer_nr].input_image_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].conv_kernel_size += 1\n",
    "                    return f\"conv kernel += 1 of 2d block no. {layer_nr}\"\n",
    "            else:\n",
    "                if dna.blocks_2d_gene[layer_nr].pool_kernel_size < dna.blocks_2d_gene[layer_nr].after_conv_image_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].pool_kernel_size += 1\n",
    "                    return f\"pool kernel +=1 of 2d block no. {layer_nr}\"\n",
    "    def decrease_kernel(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_decrease_kernel < self.R:\n",
    "            if len(dna.blocks_2d_gene) == 0: return None\n",
    "            layer_nr = random.randrange(0, len(dna.blocks_2d_gene))\n",
    "            if random.random() > .5:\n",
    "                if dna.blocks_2d_gene[layer_nr].conv_kernel_size > 1 and dna.blocks_2d_gene[layer_nr].conv_padding*2 < dna.blocks_2d_gene[layer_nr].conv_kernel_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].conv_kernel_size -= 1\n",
    "                    return f\"conv kernel -= 1 of 2d block no. {layer_nr}\"\n",
    "            else:\n",
    "                if dna.blocks_2d_gene[layer_nr].pool_kernel_size > 1 and dna.blocks_2d_gene[layer_nr].pool_padding*2 < dna.blocks_2d_gene[layer_nr].pool_kernel_size:\n",
    "                    dna.blocks_2d_gene[layer_nr].pool_kernel_size -= 1\n",
    "                    return f\"pool kernel -= 1 of 2d block no. {layer_nr}\"\n",
    "    def change_optimizer(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_change_optimizer < self.R:\n",
    "            optimizer_index = random.randrange(0, len(NN_dna.optimizer_dict))\n",
    "            dna.optimizer_gene = optimizer_index\n",
    "            return f\"changed optimizer to {NN_dna.optimizer_dict[optimizer_index]}\"\n",
    "    def change_loss_fn(self, dna: NN_dna, p_raw: float = -1):\n",
    "        if p_raw * self.impact_change_loss_fn < self.R:\n",
    "            loss_fn_index = random.randrange(0, len(NN_dna.loss_fn_dict))\n",
    "            dna.loss_fn_gene = loss_fn_index\n",
    "            return f\"changed loss function to {NN_dna.loss_fn_dict[loss_fn_index]}\"\n",
    "\n",
    "def mutate_dna(dna: NN_dna, print_actions: bool = True, mutant_name = \"NN\") -> NN_dna:\n",
    "    # create a mutation instance (this produces a radicality R)\n",
    "    m = Mutation()\n",
    "    if print_actions: print(f\"Radicality in creation of '{mutant_name}': {m.R:.2g}\")\n",
    "\n",
    "    # Dynamically loop over all functions of the Mutation class\n",
    "    for method_name in dir(m): # Loop through all attributes of the class\n",
    "        if callable(getattr(m, method_name)) and not method_name.startswith(\"__\"): # Filter to only functions (ignoring private methods and attributes)\n",
    "            p_raw = random.random() # raw likelihood\n",
    "            operation = getattr(m, method_name)\n",
    "            effect = operation(dna, p_raw) # execute the operation\n",
    "            if effect != None and print_actions: print(f\"- {effect}\")\n",
    "    return dna # return new dna to simplify usage of this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP Run\n",
    "1. Create a random population of given POP_SIZE\n",
    "2. `train_and_evaluate_gen` for one epoch\n",
    "3. choose the half of `NN_individual`s that has the best `running_acc`\n",
    "4. mutate each chosen NN **twice**\n",
    "5. continue at *2.* unless you are already at gen FINAL_GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Gen. 1 / 4 *****\n",
      "*** Commencing epoch 1 / 1 for 6 individuals, one line each. ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1. NN0: 231it [00:01, 142.88it/s]"
     ]
    }
   ],
   "source": [
    "POP_SIZE = 6\n",
    "FINAL_GEN = 4\n",
    "MAX_2D_BLOCK_COUNT = 7\n",
    "MAX_KERNEL_SIZE = 11\n",
    "\n",
    "testing_interval = 300\n",
    "epochs = 1\n",
    "testing_data_fraction = .5\n",
    "\n",
    "gens = []\n",
    "pop = create_random_population(POP_SIZE, max_2d_block_count=MAX_2D_BLOCK_COUNT, max_kernel_size=MAX_KERNEL_SIZE, name_prefix=\"NN\", device=device, print_summary=False)\n",
    "gens.append(pop)\n",
    "for gen in range(FINAL_GEN):\n",
    "    print(f\"***** Gen. {gen+1} / {FINAL_GEN} *****\")\n",
    "    train_and_evaluate_gen(pop, train_dl_f_mnist, test_dl_f_mnist, testing_interval, testing_data_fraction, epochs=epochs, live_plot=False, only_last_plot=True)\n",
    "    # only let the top half (w.r.t. running_acc) survive\n",
    "    survivors = sorted(pop, key=lambda ind: ind.running_acc, reverse=True)[:len(pop) // 2]\n",
    "    # mutate each survivor twice\n",
    "    pop = NN_population([NN_individual(mutate_dna(ind.dna, mutant_name=ind.name+\".\"+v), name=ind.name+\".\"+v, device=device) for ind in sorted(survivors, key=lambda ind: ind.name) for v in ['a','b']])\n",
    "    gens.append(pop)\n",
    "train_and_evaluate_gen(pop, train_dl_f_mnist, test_dl_f_mnist, testing_interval, testing_data_fraction, epochs=epochs, live_plot=False, only_last_plot=True)\n",
    "print(f\"******* SUMMARY *******\")\n",
    "sorted(NN_population([gens[i][j] for i in range(len(gens)) for j in range(len(gens[i]))]), key=lambda ind: ind.running_acc).plot_accs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_channels = 5 <-- (28 x 28)\n",
      "conv_2d (kernel, stride, padding) =\t(3, 3, 0) --> (9 x 9)\n",
      "max_pool_2d (kernel, stride, padding) =\t(2, 1, 0) --> (9 x 9)\n",
      "\n",
      "out_channels = 12 <-- (9 x 9)\n",
      "conv_2d (kernel, stride, padding) =\t(3, 4, 0) --> (1 x 1)\n",
      "max_pool_2d (kernel, stride, padding) =\t(0, 1, 0) --> (6 x 6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying to spot the problem\n",
    "ind = [ind for ind in pop.individuals if ind.name==\"NN1.a.b\"][0]\n",
    "ind2 = [ind for ind in gens[0].individuals if ind.name==\"NN0\"][0]\n",
    "for block in ind.dna.blocks_2d_gene:\n",
    "    print(f\"{block.toString()}\\n\")\n",
    "#ind, ind2\n",
    "# problem: a adding/deleting 2d blocks messes up the input_image_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
